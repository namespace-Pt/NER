{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitnnconda048fc75fe4ee43f1aa97608c8881ebba",
   "display_name": "Python 3.8.5 64-bit ('nn': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.bert_model import BERT_BASE\n",
    "from utils.utils import prepare, predict, train, evaluate\n",
    "\n",
    "hparams = {\n",
    "    # 'path':'data/data_test_en.txt', \n",
    "    'path':'data/data.txt',\n",
    "    'epochs': 100,\n",
    "    'batch_size': 32,\n",
    "    'embedding_dim': 768,\n",
    "    'device':'cuda:1',\n",
    "    'bert':'bert-base-chinese',\n",
    "    'seq_length':256,\n",
    "}\n",
    "\n",
    "attr_dict, loaders = prepare(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_model = BERT_BASE(hparams, attr_dict['tag2idx']).to(hparams['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch 1 , step 27 , loss: 1.8697: : 28it [00:14,  1.90it/s]\n",
      "{'macro_f1': 0.1302, 'weighted_f1': 0.1063}\n",
      "epoch 2 , step 27 , loss: 1.1396: : 28it [00:14,  1.97it/s]\n",
      "{'macro_f1': 0.2443, 'weighted_f1': 0.1348}\n",
      "epoch 3 , step 27 , loss: 0.8677: : 28it [00:14,  1.96it/s]\n",
      "{'macro_f1': 0.2564, 'weighted_f1': 0.178}\n",
      "epoch 4 , step 27 , loss: 0.7036: : 28it [00:14,  1.95it/s]\n",
      "{'macro_f1': 0.2705, 'weighted_f1': 0.195}\n",
      "epoch 5 , step 27 , loss: 0.5870: : 28it [00:14,  1.95it/s]\n",
      "{'macro_f1': 0.2852, 'weighted_f1': 0.2373}\n",
      "epoch 6 , step 27 , loss: 0.4872: : 28it [00:14,  1.95it/s]\n",
      "{'macro_f1': 0.2823, 'weighted_f1': 0.238}\n",
      "epoch 7 , step 27 , loss: 0.4124: : 28it [00:14,  1.95it/s]\n",
      "{'macro_f1': 0.2827, 'weighted_f1': 0.2488}\n",
      "epoch 8 , step 27 , loss: 0.3563: : 28it [00:14,  1.94it/s]\n",
      "{'macro_f1': 0.3036, 'weighted_f1': 0.2684}\n",
      "epoch 9 , step 27 , loss: 0.3128: : 28it [00:14,  1.95it/s]\n",
      "{'macro_f1': 0.2915, 'weighted_f1': 0.2574}\n",
      "epoch 10 , step 27 , loss: 0.2829: : 28it [00:14,  1.95it/s]\n",
      "{'macro_f1': 0.3248, 'weighted_f1': 0.2981}\n",
      "epoch 11 , step 27 , loss: 0.2557: : 28it [00:14,  1.95it/s]\n",
      "{'macro_f1': 0.3135, 'weighted_f1': 0.2932}\n",
      "epoch 12 , step 27 , loss: 0.2208: : 28it [00:14,  1.96it/s]\n",
      "{'macro_f1': 0.3173, 'weighted_f1': 0.3027}\n",
      "epoch 13 , step 27 , loss: 0.1941: : 28it [00:14,  1.96it/s]\n",
      "{'macro_f1': 0.3245, 'weighted_f1': 0.3071}\n",
      "epoch 14 , step 27 , loss: 0.1725: : 28it [00:14,  1.96it/s]\n",
      "{'macro_f1': 0.3311, 'weighted_f1': 0.3296}\n",
      "epoch 15 , step 27 , loss: 0.1583: : 28it [00:14,  1.95it/s]\n",
      "{'macro_f1': 0.3279, 'weighted_f1': 0.3278}\n",
      "epoch 16 , step 27 , loss: 0.1348: : 28it [00:14,  1.95it/s]\n",
      "{'macro_f1': 0.3231, 'weighted_f1': 0.3476}\n",
      "epoch 17 , step 27 , loss: 0.1194: : 28it [00:14,  1.96it/s]\n",
      "{'macro_f1': 0.3154, 'weighted_f1': 0.3333}\n",
      "epoch 18 , step 27 , loss: 0.1095: : 28it [00:13,  2.00it/s]\n",
      "{'macro_f1': 0.3246, 'weighted_f1': 0.3344}\n",
      "epoch 19 , step 27 , loss: 0.1090: : 28it [00:14,  1.95it/s]\n",
      "{'macro_f1': 0.3295, 'weighted_f1': 0.3872}\n",
      "epoch 20 , step 27 , loss: 0.0931: : 28it [00:14,  1.96it/s]\n",
      "{'macro_f1': 0.3154, 'weighted_f1': 0.3571}\n",
      "epoch 21 , step 27 , loss: 0.0780: : 28it [00:14,  1.94it/s]\n",
      "{'macro_f1': 0.3158, 'weighted_f1': 0.3487}\n",
      "epoch 22 , step 27 , loss: 0.0666: : 28it [00:14,  1.96it/s]\n",
      "{'macro_f1': 0.3151, 'weighted_f1': 0.3519}\n",
      "epoch 23 , step 27 , loss: 0.0566: : 28it [00:14,  1.96it/s]\n",
      "{'macro_f1': 0.3163, 'weighted_f1': 0.3642}\n",
      "epoch 24 , step 27 , loss: 0.0503: : 28it [00:14,  1.95it/s]\n",
      "{'macro_f1': 0.3252, 'weighted_f1': 0.3782}\n",
      "epoch 25 , step 1 , loss: 0.0358: : 2it [00:01,  1.81it/s]"
     ]
    }
   ],
   "source": [
    "bert_model = train(hparams, bert_model, loaders, lr=2e-5, schedule=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch 1 , step 27 , loss: 278.0432: : 28it [00:14,  1.99it/s]\n",
      "{'macro_f1': 0.1229, 'weighted_f1': 0.7902}\n",
      "epoch 2 , step 27 , loss: 154.2407: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.1944, 'weighted_f1': 0.8211}\n",
      "epoch 3 , step 27 , loss: 123.6196: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.2655, 'weighted_f1': 0.8413}\n",
      "epoch 4 , step 27 , loss: 103.6186: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.3444, 'weighted_f1': 0.864}\n",
      "epoch 5 , step 27 , loss: 88.2204: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.3718, 'weighted_f1': 0.872}\n",
      "epoch 6 , step 27 , loss: 75.3234: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.3887, 'weighted_f1': 0.8786}\n",
      "epoch 7 , step 27 , loss: 65.2006: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4114, 'weighted_f1': 0.8846}\n",
      "epoch 8 , step 27 , loss: 59.3067: : 28it [00:13,  2.05it/s]\n",
      "{'macro_f1': 0.4232, 'weighted_f1': 0.888}\n",
      "epoch 9 , step 27 , loss: 53.3247: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4249, 'weighted_f1': 0.8895}\n",
      "epoch 10 , step 27 , loss: 49.1286: : 28it [00:13,  2.05it/s]\n",
      "{'macro_f1': 0.4296, 'weighted_f1': 0.8886}\n",
      "epoch 11 , step 27 , loss: 43.0126: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4367, 'weighted_f1': 0.8912}\n",
      "epoch 12 , step 27 , loss: 37.9973: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.44, 'weighted_f1': 0.8911}\n",
      "epoch 13 , step 27 , loss: 34.1573: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4465, 'weighted_f1': 0.8904}\n",
      "epoch 14 , step 27 , loss: 30.7700: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4559, 'weighted_f1': 0.8938}\n",
      "epoch 15 , step 27 , loss: 28.4128: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4592, 'weighted_f1': 0.893}\n",
      "epoch 16 , step 27 , loss: 26.5242: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4594, 'weighted_f1': 0.8933}\n",
      "epoch 17 , step 27 , loss: 24.3637: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4623, 'weighted_f1': 0.8924}\n",
      "epoch 18 , step 27 , loss: 22.2116: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4816, 'weighted_f1': 0.8943}\n",
      "epoch 19 , step 27 , loss: 20.3368: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4698, 'weighted_f1': 0.8932}\n",
      "epoch 20 , step 27 , loss: 18.7555: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4773, 'weighted_f1': 0.8938}\n",
      "epoch 21 , step 27 , loss: 17.0416: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4815, 'weighted_f1': 0.8927}\n",
      "epoch 22 , step 27 , loss: 15.8609: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4834, 'weighted_f1': 0.8935}\n",
      "epoch 23 , step 27 , loss: 14.5502: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4901, 'weighted_f1': 0.8943}\n",
      "epoch 24 , step 27 , loss: 13.4182: : 28it [00:13,  2.05it/s]\n",
      "{'macro_f1': 0.4863, 'weighted_f1': 0.8932}\n",
      "epoch 25 , step 27 , loss: 11.9353: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4912, 'weighted_f1': 0.8934}\n",
      "epoch 26 , step 27 , loss: 10.7851: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4899, 'weighted_f1': 0.8938}\n",
      "epoch 27 , step 27 , loss: 10.2726: : 28it [00:13,  2.05it/s]\n",
      "{'macro_f1': 0.472, 'weighted_f1': 0.892}\n",
      "epoch 28 , step 27 , loss: 10.0735: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4838, 'weighted_f1': 0.8932}\n",
      "epoch 29 , step 27 , loss: 9.5548: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4949, 'weighted_f1': 0.8922}\n",
      "epoch 30 , step 27 , loss: 9.8634: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4863, 'weighted_f1': 0.8932}\n",
      "epoch 31 , step 27 , loss: 9.3956: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.475, 'weighted_f1': 0.8909}\n",
      "epoch 32 , step 27 , loss: 9.2082: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4864, 'weighted_f1': 0.8929}\n",
      "epoch 33 , step 27 , loss: 7.8277: : 28it [00:13,  2.05it/s]\n",
      "{'macro_f1': 0.4855, 'weighted_f1': 0.8922}\n",
      "epoch 34 , step 27 , loss: 7.0723: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4768, 'weighted_f1': 0.892}\n",
      "epoch 35 , step 27 , loss: 6.5759: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4745, 'weighted_f1': 0.8913}\n",
      "epoch 36 , step 27 , loss: 5.9582: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4791, 'weighted_f1': 0.8922}\n",
      "epoch 37 , step 27 , loss: 5.5606: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4796, 'weighted_f1': 0.8917}\n",
      "epoch 38 , step 27 , loss: 5.1721: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4777, 'weighted_f1': 0.8914}\n",
      "epoch 39 , step 27 , loss: 4.8549: : 28it [00:13,  2.04it/s]\n",
      "{'macro_f1': 0.477, 'weighted_f1': 0.8917}\n",
      "epoch 40 , step 27 , loss: 4.5935: : 28it [00:13,  2.08it/s]\n",
      "{'macro_f1': 0.4728, 'weighted_f1': 0.8916}\n",
      "epoch 41 , step 27 , loss: 4.4497: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4762, 'weighted_f1': 0.8912}\n",
      "epoch 42 , step 27 , loss: 4.3234: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.474, 'weighted_f1': 0.8913}\n",
      "epoch 43 , step 27 , loss: 4.1554: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4742, 'weighted_f1': 0.891}\n",
      "epoch 44 , step 27 , loss: 4.1244: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4737, 'weighted_f1': 0.8908}\n",
      "epoch 45 , step 27 , loss: 3.9828: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4746, 'weighted_f1': 0.8905}\n",
      "epoch 46 , step 27 , loss: 3.9045: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4734, 'weighted_f1': 0.8906}\n",
      "epoch 47 , step 27 , loss: 3.8011: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4733, 'weighted_f1': 0.8908}\n",
      "epoch 48 , step 27 , loss: 3.7474: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.473, 'weighted_f1': 0.8903}\n",
      "epoch 49 , step 27 , loss: 3.6765: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4744, 'weighted_f1': 0.8903}\n",
      "epoch 50 , step 27 , loss: 3.6032: : 28it [00:13,  2.08it/s]\n",
      "{'macro_f1': 0.4751, 'weighted_f1': 0.8906}\n",
      "epoch 51 , step 27 , loss: 3.5103: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.475, 'weighted_f1': 0.8903}\n",
      "epoch 52 , step 27 , loss: 3.4519: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4735, 'weighted_f1': 0.8903}\n",
      "epoch 53 , step 27 , loss: 3.4303: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4733, 'weighted_f1': 0.8901}\n",
      "epoch 54 , step 27 , loss: 3.3605: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4743, 'weighted_f1': 0.89}\n",
      "epoch 55 , step 27 , loss: 3.2900: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4729, 'weighted_f1': 0.8901}\n",
      "epoch 56 , step 27 , loss: 3.2624: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4758, 'weighted_f1': 0.8904}\n",
      "epoch 57 , step 27 , loss: 3.2160: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4722, 'weighted_f1': 0.8896}\n",
      "epoch 58 , step 27 , loss: 3.1228: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4766, 'weighted_f1': 0.8904}\n",
      "epoch 59 , step 27 , loss: 3.1048: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4731, 'weighted_f1': 0.8899}\n",
      "epoch 60 , step 27 , loss: 3.0513: : 28it [00:13,  2.08it/s]\n",
      "{'macro_f1': 0.4732, 'weighted_f1': 0.89}\n",
      "epoch 61 , step 27 , loss: 3.0129: : 28it [00:13,  2.05it/s]\n",
      "{'macro_f1': 0.4712, 'weighted_f1': 0.8896}\n",
      "epoch 62 , step 27 , loss: 2.9065: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4738, 'weighted_f1': 0.8902}\n",
      "epoch 63 , step 27 , loss: 2.9005: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4728, 'weighted_f1': 0.8902}\n",
      "epoch 64 , step 27 , loss: 2.8140: : 28it [00:13,  2.05it/s]\n",
      "{'macro_f1': 0.4729, 'weighted_f1': 0.8896}\n",
      "epoch 65 , step 27 , loss: 2.7799: : 28it [00:13,  2.05it/s]\n",
      "{'macro_f1': 0.4749, 'weighted_f1': 0.8901}\n",
      "epoch 66 , step 27 , loss: 2.7399: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4743, 'weighted_f1': 0.89}\n",
      "epoch 67 , step 27 , loss: 2.6974: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4744, 'weighted_f1': 0.8899}\n",
      "epoch 68 , step 27 , loss: 2.6732: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4751, 'weighted_f1': 0.8901}\n",
      "epoch 69 , step 27 , loss: 2.6478: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4753, 'weighted_f1': 0.8899}\n",
      "epoch 70 , step 27 , loss: 2.5799: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4748, 'weighted_f1': 0.8901}\n",
      "epoch 71 , step 27 , loss: 2.5641: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4757, 'weighted_f1': 0.89}\n",
      "epoch 72 , step 27 , loss: 2.5358: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.473, 'weighted_f1': 0.8897}\n",
      "epoch 73 , step 27 , loss: 2.5094: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4755, 'weighted_f1': 0.89}\n",
      "epoch 74 , step 27 , loss: 2.4969: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4772, 'weighted_f1': 0.8902}\n",
      "epoch 75 , step 27 , loss: 2.4544: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4743, 'weighted_f1': 0.8899}\n",
      "epoch 76 , step 27 , loss: 2.4432: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4734, 'weighted_f1': 0.8897}\n",
      "epoch 77 , step 27 , loss: 2.4026: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4757, 'weighted_f1': 0.8899}\n",
      "epoch 78 , step 27 , loss: 2.3880: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4757, 'weighted_f1': 0.8901}\n",
      "epoch 79 , step 27 , loss: 2.3572: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4753, 'weighted_f1': 0.8901}\n",
      "epoch 80 , step 27 , loss: 2.3754: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4755, 'weighted_f1': 0.8899}\n",
      "epoch 81 , step 27 , loss: 2.3165: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4781, 'weighted_f1': 0.8902}\n",
      "epoch 82 , step 27 , loss: 2.3079: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4751, 'weighted_f1': 0.8902}\n",
      "epoch 83 , step 27 , loss: 2.2910: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4745, 'weighted_f1': 0.8899}\n",
      "epoch 84 , step 27 , loss: 2.2684: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4755, 'weighted_f1': 0.8902}\n",
      "epoch 85 , step 27 , loss: 2.2500: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4755, 'weighted_f1': 0.8901}\n",
      "epoch 86 , step 27 , loss: 2.2455: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4754, 'weighted_f1': 0.8899}\n",
      "epoch 87 , step 27 , loss: 2.2319: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.475, 'weighted_f1': 0.8899}\n",
      "epoch 88 , step 27 , loss: 2.2340: : 28it [00:13,  2.05it/s]\n",
      "{'macro_f1': 0.4761, 'weighted_f1': 0.8904}\n",
      "epoch 89 , step 27 , loss: 2.2050: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4774, 'weighted_f1': 0.8903}\n",
      "epoch 90 , step 27 , loss: 2.1948: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4766, 'weighted_f1': 0.8901}\n",
      "epoch 91 , step 27 , loss: 2.1761: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4768, 'weighted_f1': 0.8901}\n",
      "epoch 92 , step 27 , loss: 2.1784: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4776, 'weighted_f1': 0.8905}\n",
      "epoch 93 , step 27 , loss: 2.1810: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4768, 'weighted_f1': 0.8901}\n",
      "epoch 94 , step 27 , loss: 2.1698: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4771, 'weighted_f1': 0.8902}\n",
      "epoch 95 , step 27 , loss: 2.1485: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4767, 'weighted_f1': 0.8903}\n",
      "epoch 96 , step 27 , loss: 2.1463: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4766, 'weighted_f1': 0.8903}\n",
      "epoch 97 , step 27 , loss: 2.1455: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4776, 'weighted_f1': 0.8904}\n",
      "epoch 98 , step 27 , loss: 2.1274: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.4771, 'weighted_f1': 0.8903}\n",
      "epoch 99 , step 27 , loss: 2.1383: : 28it [00:13,  2.07it/s]\n",
      "{'macro_f1': 0.4773, 'weighted_f1': 0.8904}\n",
      "epoch 100 , step 27 , loss: 2.1299: : 28it [00:13,  2.06it/s]\n",
      "{'macro_f1': 0.477, 'weighted_f1': 0.8903}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils.bert_model import BERT_NER\n",
    "from utils.utils import prepare, predict, train, evaluate\n",
    "\n",
    "hparams = {\n",
    "    # 'path':'data/data_test_en.txt', \n",
    "    'path':'data/data.txt',\n",
    "    'epochs': 100,\n",
    "    'batch_size': 32,\n",
    "    'embedding_dim': 768,\n",
    "    'device':'cuda:1',\n",
    "    # 'bert':'dslim/bert-base-NER', \n",
    "    'bert': 'ckiplab/bert-base-chinese-ner',\n",
    "    'seq_length': 256,\n",
    "    'learning_rate': 3e-5\n",
    "}\n",
    "attr_dict, loaders = prepare(hparams)\n",
    "\n",
    "bert_model = BERT_NER(hparams, attr_dict['tag2idx']).to(hparams['device'])\n",
    "\n",
    "bert_model = train(hparams, bert_model, loaders, lr=hparams['learning_rate'], schedule=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}