{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitnnconda048fc75fe4ee43f1aa97608c8881ebba",
   "display_name": "Python 3.8.5 64-bit ('nn': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from utils.bert_model import BiLSTM_CRF_BERT\n",
    "from utils.utils import prepare, predict\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "from transformers import AutoModel,BertTokenizerFast, BertTokenizer,BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'path':'/home/peitian_zhang/Data/NER/labeled_train.txt',\n",
    "    'epochs': 50,\n",
    "    'batch_size': 15,\n",
    "    'embedding_dim': 768,\n",
    "    'hidden_dim': 768,\n",
    "    'device':'cuda:0',\n",
    "    'bert':'ckiplab/albert-base-chinese',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2idx, vocab, loader = prepare(hparams)\n",
    "hparams['vocab_size'] = len(vocab)\n",
    "hparams['seq_length'] = loader.dataset.max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of AlbertModel were not initialized from the model checkpoint at ckiplab/albert-base-chinese and are newly initialized: ['albert.pooler.weight', 'albert.pooler.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_model = BiLSTM_CRF_BERT(hparams,tag2idx).to(hparams['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch 1 , step 64 , loss: -9085.8949: : 65it [00:47,  1.36it/s]\n",
      "epoch 2 , step 64 , loss: -31231.9765: : 65it [00:47,  1.36it/s]\n",
      "epoch 3 , step 64 , loss: -52123.6125: : 65it [00:46,  1.39it/s]\n",
      "epoch 4 , step 64 , loss: -72145.6463: : 65it [00:47,  1.37it/s]\n",
      "epoch 5 , step 64 , loss: -91053.4327: : 65it [00:47,  1.37it/s]\n",
      "epoch 6 , step 64 , loss: -108740.0206: : 65it [00:48,  1.34it/s]\n",
      "epoch 7 , step 64 , loss: -125400.5459: : 65it [00:48,  1.35it/s]\n",
      "epoch 8 , step 64 , loss: -140919.7337: : 65it [00:47,  1.36it/s]\n",
      "epoch 9 , step 64 , loss: -155171.3752: : 65it [00:47,  1.36it/s]\n",
      "epoch 10 , step 64 , loss: -168184.3171: : 65it [00:47,  1.37it/s]\n",
      "epoch 11 , step 64 , loss: -179987.4394: : 65it [00:47,  1.37it/s]\n",
      "epoch 12 , step 64 , loss: -190501.6435: : 65it [00:48,  1.34it/s]\n",
      "epoch 13 , step 64 , loss: -199911.0161: : 65it [00:47,  1.37it/s]\n",
      "epoch 14 , step 64 , loss: -208174.5748: : 65it [00:46,  1.38it/s]\n",
      "epoch 15 , step 64 , loss: -215236.1685: : 65it [00:48,  1.35it/s]\n",
      "epoch 16 , step 64 , loss: -221128.8661: : 65it [00:48,  1.35it/s]\n",
      "epoch 17 , step 64 , loss: -225846.2541: : 65it [00:47,  1.36it/s]\n",
      "epoch 18 , step 64 , loss: -229387.5046: : 65it [00:48,  1.35it/s]\n",
      "epoch 19 , step 64 , loss: -231765.2062: : 65it [00:47,  1.36it/s]\n",
      "epoch 20 , step 64 , loss: -232961.3096: : 65it [00:47,  1.36it/s]\n"
     ]
    }
   ],
   "source": [
    "total_steps = len(loader) * hparams['epochs']\n",
    "\n",
    "optimizer = optim.AdamW(bert_model.parameters(),lr=0.001)\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n",
    "\n",
    "for epoch in range(hparams['epochs']):\n",
    "    tqdm_ = tqdm(enumerate(loader))\n",
    "    total_loss = 0\n",
    "\n",
    "    for step,x in tqdm_:\n",
    "        bert_model.zero_grad()\n",
    "        loss = bert_model.neg_log_likelihood(x)\n",
    "        loss.backward()\n",
    "\n",
    "        # prevent gradient explosion\n",
    "        # torch.nn.utils.clip_grad_norm_(bert_model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        tqdm_.set_description(\"epoch {:d} , step {:d} , loss: {:.4f}\".format(epoch+1, step, total_loss/(step+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prediction:tensor([[18,  3,  3,  ...,  0, 18,  3],\n        [18,  3,  3,  ...,  0, 18,  3],\n        [18,  3,  3,  ...,  0, 18,  3],\n        ...,\n        [18,  3,  3,  ...,  0, 18,  3],\n        [18,  3,  3,  ...,  0, 18,  3],\n        [18,  3,  3,  ...,  0, 18,  3]], device='cuda:0')\n Ground Truth:tensor([[ 3,  3,  3,  ...,  2,  2,  2],\n        [ 3,  3,  3,  ...,  2,  2,  2],\n        [14, 15, 15,  ...,  2,  2,  2],\n        ...,\n        [ 3,  3,  3,  ...,  2,  2,  2],\n        [ 3,  3,  3,  ...,  2,  2,  2],\n        [ 3,  3,  3,  ...,  2,  2,  2]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    record = next(iter(loader))\n",
    "    _, tag_seq = bert_model(record['token'], record['attn_mask'])\n",
    "    print(\"Prediction:{}\\n Ground Truth:{}\".format(tag_seq, record['label']))"
   ]
  },
  {
   "source": [
    "below is still under development"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel,AutoModel\n",
    "\n",
    "START_TAG = '<START>'\n",
    "STOP_TAG = '<END>'\n",
    "PAD_TAG = '<PAD>'\n",
    "\n",
    "class BERT_NER(nn.Module):\n",
    "    \"\"\"\n",
    "        fine-tune pretrained BERT-NER model on our dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparams, tag2idx):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = hparams['batch_size']\n",
    "        self.seq_length = hparams['seq_length']\n",
    "\n",
    "        self.device = hparams['device']\n",
    "\n",
    "        self.tag2idx = tag2idx\n",
    "        self.tagset_size = len(tag2idx)\n",
    "        self.idx2tag = {v:k for k,v in tag2idx.items()}\n",
    "\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim // 2, bidirectional=True)\n",
    "\n",
    "        self.bert = AutoModel.from_pretrained(\n",
    "            'ckiplab/albert-base-chinese-ner',\n",
    "            # output hidden embedding of each transformer layer\n",
    "        )\n",
    "\n",
    "        self.hidden2tag = nn.Linear(self.embedding_dim, self.tagset_size)\n",
    "        self.Softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def _bert_encode(token, attn_mask):\n",
    "        output = self.bert(token, token_mask)\n",
    "        embedding = output['last_hidden_state']\n",
    "        tag_prob = self.Softmax(self.hidden2tag(embedding))\n",
    "\n",
    "    \n",
    "    def forward(x):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of AlbertModel were not initialized from the model checkpoint at ckiplab/albert-base-chinese and are newly initialized: ['albert.pooler.weight', 'albert.pooler.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained('ckiplab/albert-base-chinese', output_hidden_states=True)\n",
    "# a = model(encoded['input_ids'], encoded['attention_mask'], labels=torch.tensor([[14,15,15,3,3,2,2,2,2,2]]))\n",
    "a = model(encoded['input_ids'], encoded['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}