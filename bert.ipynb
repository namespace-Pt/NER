{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitnnconda048fc75fe4ee43f1aa97608c8881ebba",
   "display_name": "Python 3.8.5 64-bit ('nn': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Base BERT with second-to-last pooling for word representation, then apply CRF to calculate sentence score, and minimize the negative log likelihood to train"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.bert_model import BERT_CRF\n",
    "from utils.utils import prepare, predict, train, evaluate\n",
    "\n",
    "hparams = {\n",
    "    # 'path':'/home/peitian_zhang/Data/NER/test.txt', \n",
    "    'path':'/home/peitian_zhang/Data/NER/labeled_train.txt',\n",
    "    'epochs': 10,\n",
    "    'batch_size': 40,\n",
    "    'embedding_dim': 768,\n",
    "    'device':'cuda:1',\n",
    "    'bert':'bert-base-chinese',\n",
    "    'seq_length':256,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_dict, loaders = prepare(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BERT_CRF(hparams,attr_dict['tag2idx']).to(hparams['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch 1 , step 21 , loss: 608.7438: : 22it [00:18,  1.19it/s]\n",
      "{'weighted_f1': 0.5574, 'micro_f1': 0.6853, 'macro_f1': 0.0407}\n",
      "epoch 2 , step 21 , loss: 306.9480: : 22it [00:17,  1.23it/s]\n",
      "{'weighted_f1': 0.5574, 'micro_f1': 0.6853, 'macro_f1': 0.0407}\n",
      "epoch 3 , step 21 , loss: 298.9526: : 22it [00:17,  1.24it/s]\n",
      "{'weighted_f1': 0.5574, 'micro_f1': 0.6853, 'macro_f1': 0.0407}\n",
      "epoch 4 , step 21 , loss: 296.3358: : 22it [00:17,  1.24it/s]\n",
      "{'weighted_f1': 0.5574, 'micro_f1': 0.6853, 'macro_f1': 0.0407}\n",
      "epoch 5 , step 21 , loss: 291.2973: : 22it [00:17,  1.25it/s]\n",
      "{'weighted_f1': 0.5574, 'micro_f1': 0.6853, 'macro_f1': 0.0407}\n",
      "epoch 6 , step 21 , loss: 285.9999: : 22it [00:17,  1.25it/s]\n",
      "{'weighted_f1': 0.5574, 'micro_f1': 0.6853, 'macro_f1': 0.0407}\n",
      "epoch 7 , step 21 , loss: 283.4419: : 22it [00:17,  1.25it/s]\n",
      "{'weighted_f1': 0.5574, 'micro_f1': 0.6853, 'macro_f1': 0.0407}\n",
      "epoch 8 , step 21 , loss: 278.8362: : 22it [00:17,  1.24it/s]\n",
      "{'weighted_f1': 0.5574, 'micro_f1': 0.6853, 'macro_f1': 0.0407}\n",
      "epoch 9 , step 21 , loss: 273.2600: : 22it [00:17,  1.24it/s]\n",
      "{'weighted_f1': 0.5574, 'micro_f1': 0.6853, 'macro_f1': 0.0407}\n",
      "epoch 10 , step 21 , loss: 270.0124: : 22it [00:17,  1.25it/s]\n",
      "{'weighted_f1': 0.5574, 'micro_f1': 0.6853, 'macro_f1': 0.0407}\n"
     ]
    }
   ],
   "source": [
    "bert_model = train(hparams, bert_model, loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(bert_model, loaders[0])"
   ]
  },
  {
   "source": [
    "### Built-in BertForTokenClassification with labels input, normalize the output logits for classification, use the output loss for training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.bert_model import BERT_BASE\n",
    "from utils.utils import prepare, predict, train, evaluate\n",
    "\n",
    "hparams = {\n",
    "    'path':'/home/peitian_zhang/Data/NER/test.txt', \n",
    "    # 'path':'/home/peitian_zhang/Data/NER/labeled_train.txt',\n",
    "    'epochs': 10,\n",
    "    'batch_size': 30,\n",
    "    'embedding_dim': 768,\n",
    "    'device':'cuda:0',\n",
    "    'bert':'bert-base-uncased',\n",
    "    'seq_length':25,\n",
    "}\n",
    "\n",
    "attr_dict, loaders = prepare(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BERT_BASE(hparams, attr_dict['tag2idx']).to(hparams['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = train(hparams, bert_model, loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(bert_model, loaders[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(loaders[0]))"
   ]
  },
  {
   "source": [
    "### NER pretrained BERT with last hidden state pooling, then directly map the 768 dimensional hidden states to the tagset space, minimize the negative log likelihood for classification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.bert_model import BERT_NER\n",
    "from utils.utils import prepare, predict, train, evaluate\n",
    "\n",
    "hparams = {\n",
    "    'path':'/home/peitian_zhang/Data/NER/test.txt', \n",
    "    # 'path':'/home/peitian_zhang/Data/NER/labeled_train.txt',\n",
    "    'epochs': 25,\n",
    "    'batch_size': 30,\n",
    "    'embedding_dim': 768,\n",
    "    'device':'cuda:0',\n",
    "    'bert':'dslim/bert-base-NER', #ckiplab/bert-base-chinese-ner\n",
    "    'seq_length':30,\n",
    "}\n",
    "attr_dict, loaders = prepare(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BERT_NER(hparams, attr_dict['tag2idx']).to(hparams['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch 1 , step 0 , loss: 97.4674: : 1it [00:00,  3.98it/s]\n",
      "{'weighted_f1': 0.1263, 'micro_f1': 0.2667, 'macro_f1': 0.0351}\n",
      "epoch 2 , step 0 , loss: 73.6373: : 1it [00:00,  4.01it/s]\n",
      "{'weighted_f1': 0.1153, 'micro_f1': 0.2667, 'macro_f1': 0.0393}\n",
      "epoch 3 , step 0 , loss: 118.0095: : 1it [00:00,  4.32it/s]\n",
      "{'weighted_f1': 0.1385, 'micro_f1': 0.3, 'macro_f1': 0.042}\n",
      "epoch 4 , step 0 , loss: 64.1380: : 1it [00:00,  4.17it/s]\n",
      "{'weighted_f1': 0.0622, 'micro_f1': 0.0667, 'macro_f1': 0.0242}\n",
      "epoch 5 , step 0 , loss: 85.1973: : 1it [00:00,  4.27it/s]\n",
      "{'weighted_f1': 0.1385, 'micro_f1': 0.3, 'macro_f1': 0.042}\n",
      "epoch 6 , step 0 , loss: 45.5129: : 1it [00:00,  4.50it/s]\n",
      "{'weighted_f1': 0.1385, 'micro_f1': 0.3, 'macro_f1': 0.042}\n",
      "epoch 7 , step 0 , loss: 32.6179: : 1it [00:00,  3.99it/s]\n",
      "{'weighted_f1': 0.1385, 'micro_f1': 0.3, 'macro_f1': 0.042}\n",
      "epoch 8 , step 0 , loss: 34.8444: : 1it [00:00,  3.88it/s]\n",
      "{'weighted_f1': 0.1123, 'micro_f1': 0.2667, 'macro_f1': 0.0383}\n",
      "epoch 9 , step 0 , loss: 60.1739: : 1it [00:00,  3.98it/s]\n",
      "{'weighted_f1': 0.1385, 'micro_f1': 0.3, 'macro_f1': 0.042}\n",
      "epoch 10 , step 0 , loss: 35.7978: : 1it [00:00,  4.11it/s]\n",
      "{'weighted_f1': 0.1385, 'micro_f1': 0.3, 'macro_f1': 0.042}\n",
      "epoch 11 , step 0 , loss: 31.4130: : 1it [00:00,  4.57it/s]\n",
      "{'weighted_f1': 0.1385, 'micro_f1': 0.3, 'macro_f1': 0.042}\n",
      "epoch 12 , step 0 , loss: 30.8938: : 1it [00:00,  4.44it/s]\n",
      "{'weighted_f1': 0.1385, 'micro_f1': 0.3, 'macro_f1': 0.042}\n",
      "epoch 13 , step 0 , loss: 30.8616: : 1it [00:00,  4.31it/s]\n",
      "{'weighted_f1': 0.1385, 'micro_f1': 0.3, 'macro_f1': 0.042}\n",
      "epoch 14 , step 0 , loss: 30.8154: : 1it [00:00,  4.29it/s]\n",
      "{'weighted_f1': 0.1385, 'micro_f1': 0.3, 'macro_f1': 0.042}\n",
      "epoch 15 , step 0 , loss: 30.6316: : 1it [00:00,  3.98it/s]\n",
      "{'weighted_f1': 0.1385, 'micro_f1': 0.3, 'macro_f1': 0.042}\n",
      "epoch 16 , step 0 , loss: 30.6098: : 1it [00:00,  4.12it/s]\n",
      "{'weighted_f1': 0.1385, 'micro_f1': 0.3, 'macro_f1': 0.042}\n",
      "epoch 17 , step 0 , loss: 30.4416: : 1it [00:00,  4.47it/s]\n",
      "{'weighted_f1': 0.1385, 'micro_f1': 0.3, 'macro_f1': 0.042}\n",
      "epoch 18 , step 0 , loss: 30.4110: : 1it [00:00,  4.36it/s]\n",
      "{'weighted_f1': 0.1385, 'micro_f1': 0.3, 'macro_f1': 0.042}\n",
      "epoch 19 , step 0 , loss: 30.3724: : 1it [00:00,  4.18it/s]\n",
      "{'weighted_f1': 0.1385, 'micro_f1': 0.3, 'macro_f1': 0.042}\n",
      "epoch 20 , step 0 , loss: 30.2503: : 1it [00:00,  4.67it/s]\n",
      "{'weighted_f1': 0.1385, 'micro_f1': 0.3, 'macro_f1': 0.042}\n",
      "epoch 21 , step 0 , loss: 30.2587: : 1it [00:00,  4.07it/s]\n",
      "{'weighted_f1': 0.1385, 'micro_f1': 0.3, 'macro_f1': 0.042}\n",
      "epoch 22 , step 0 , loss: 30.1788: : 1it [00:00,  4.08it/s]\n",
      "{'weighted_f1': 0.1385, 'micro_f1': 0.3, 'macro_f1': 0.042}\n",
      "epoch 23 , step 0 , loss: 30.2503: : 1it [00:00,  4.02it/s]\n",
      "{'weighted_f1': 0.1385, 'micro_f1': 0.3, 'macro_f1': 0.042}\n",
      "epoch 24 , step 0 , loss: 30.1438: : 1it [00:00,  3.91it/s]\n",
      "{'weighted_f1': 0.1385, 'micro_f1': 0.3, 'macro_f1': 0.042}\n",
      "epoch 25 , step 0 , loss: 30.1689: : 1it [00:00,  4.04it/s]\n",
      "{'weighted_f1': 0.1385, 'micro_f1': 0.3, 'macro_f1': 0.042}\n"
     ]
    }
   ],
   "source": [
    "bert_model = train(hparams, bert_model, loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           2       0.70      1.00      0.82        21\n           3       0.00      0.00      0.00         5\n          12       0.00      0.00      0.00         1\n          13       0.00      0.00      0.00         1\n          14       0.00      0.00      0.00         1\n          15       0.00      0.00      0.00         1\n\n    accuracy                           0.70        30\n   macro avg       0.12      0.17      0.14        30\nweighted avg       0.49      0.70      0.58        30\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'weighted_f1': 0.5765, 'micro_f1': 0.7, 'macro_f1': 0.1373}"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "evaluate(bert_model, loaders[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}