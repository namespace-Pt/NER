{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitnnconda048fc75fe4ee43f1aa97608c8881ebba",
   "display_name": "Python 3.8.5 64-bit ('nn': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from bert_model import BiLSTM_CRF_BERT\n",
    "from utils import prepare, predict\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'path':'/home/peitian_zhang/Data/NER/labeled_train.txt',\n",
    "    'epochs': 20,\n",
    "    'batch_size': 15,\n",
    "    'embedding_dim': 768,\n",
    "    'hidden_dim': 768,\n",
    "    'device':'cuda:0',\n",
    "    'bert':'ckiplab/albert-base-chinese',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2idx, vocab, loader = prepare(hparams)\n",
    "hparams['vocab_size'] = len(vocab)\n",
    "hparams['seq_length'] = loader.dataset.max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of AlbertModel were not initialized from the model checkpoint at ckiplab/albert-base-chinese and are newly initialized: ['albert.pooler.weight', 'albert.pooler.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_model = BiLSTM_CRF_BERT(hparams,tag2idx).to(hparams['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch 1 , step 20 , loss: -1627.4039: : 21it [00:12,  1.74it/s]"
     ]
    }
   ],
   "source": [
    "total_steps = len(loader) * hparams['epochs']\n",
    "\n",
    "optimizer = optim.AdamW(bert_model.parameters(),lr=0.001)\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n",
    "\n",
    "for epoch in range(hparams['epochs']):\n",
    "    tqdm_ = tqdm(enumerate(loader))\n",
    "    total_loss = 0\n",
    "\n",
    "    for step,x in tqdm_:\n",
    "        bert_model.zero_grad()\n",
    "        loss = bert_model.neg_log_likelihood(x)\n",
    "        loss.backward()\n",
    "\n",
    "        # prevent gradient explosion\n",
    "        # torch.nn.utils.clip_grad_norm_(bert_model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        tqdm_.set_description(\"epoch {:d} , step {:d} , loss: {:.4f}\".format(epoch+1, step, total_loss/(step+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    record = next(iter(loader))\n",
    "    _, tag_seq = bert_model(record['token'], record['attn_mask'])\n",
    "    print(\"Prediction:{}\\n Ground Truth:{}\".format(tag_seq, record['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel,BertTokenizerFast, BertTokenizer,BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "encoded = tokenizer.encode_plus('窦志成获奖', pad_to_max_length=True, max_length=10, truncation=True, return_tensors='pt')\n",
    "encoded['input_ids'], encoded['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of AlbertModel were not initialized from the model checkpoint at ckiplab/albert-base-chinese-ner and are newly initialized: ['albert.pooler.weight', 'albert.pooler.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.1606,  0.2864, -0.1147,  ..., -0.0842,  0.1854,  0.0919],\n",
       "         [ 1.6543,  0.3195, -0.3530,  ...,  0.0622, -0.7103, -0.3791],\n",
       "         [ 0.0810, -0.5529, -0.0126,  ..., -0.1332,  0.7997, -1.2569],\n",
       "         ...,\n",
       "         [-0.7667, -0.0724, -0.3125,  ..., -0.4537,  0.4974, -0.7879],\n",
       "         [-0.7574, -0.0837, -0.3519,  ..., -0.4553,  0.5126, -0.7774],\n",
       "         [-0.7646, -0.0973, -0.4095,  ..., -0.4482,  0.5277, -0.7301]]],\n",
       "       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[ 6.7524e-02,  4.7782e-02,  1.1634e-01, -1.1055e-02,  7.7158e-02,\n",
       "         -1.3929e-01,  6.3112e-02,  1.6692e-01, -9.6543e-02, -4.2006e-02,\n",
       "         -6.1763e-02, -8.4560e-02, -2.0876e-02, -5.5645e-02, -3.1521e-01,\n",
       "         -4.2518e-02, -7.5884e-02, -3.6048e-02, -8.2107e-03,  8.8506e-02,\n",
       "          3.7791e-02, -1.4655e-01, -3.5113e-02,  8.0328e-02, -7.6507e-02,\n",
       "         -3.3539e-02,  2.8168e-01,  2.8507e-01,  5.8243e-02, -5.4598e-02,\n",
       "         -6.7155e-02, -8.4662e-02, -6.2602e-02, -2.4955e-01,  2.3052e-02,\n",
       "          5.1944e-02, -1.0777e-01,  7.6104e-02,  6.0575e-03, -5.4094e-02,\n",
       "          1.3332e-01,  3.0029e-02, -7.5771e-02,  7.1567e-02,  5.3846e-02,\n",
       "          2.4717e-02, -2.8651e-02,  1.6792e-01,  4.6373e-02, -2.8309e-01,\n",
       "          1.1232e-01, -1.1353e-01, -8.9414e-02,  1.3158e-01, -1.4685e-01,\n",
       "         -5.6010e-02,  1.2082e-01, -6.9422e-02,  7.7270e-02, -3.3390e-02,\n",
       "          3.9782e-02, -1.4901e-01,  8.8333e-03, -3.6346e-02,  3.4004e-02,\n",
       "         -2.0221e-01,  7.1304e-02, -1.3564e-01, -5.6540e-02, -2.1115e-01,\n",
       "          4.5098e-02, -2.3905e-02,  2.1459e-02, -2.9820e-02,  5.7000e-02,\n",
       "          1.5658e-01, -9.0966e-02, -2.4501e-03, -4.9724e-03, -1.1467e-01,\n",
       "         -1.8430e-01, -1.0040e-01,  5.3735e-03, -1.6715e-01, -6.2666e-02,\n",
       "          9.6601e-02, -6.5957e-02, -1.8508e-01, -8.5643e-02,  3.8291e-02,\n",
       "          1.4046e-01, -7.7036e-02, -7.2501e-02,  6.5825e-02, -8.5206e-02,\n",
       "          1.8308e-01,  1.1600e-01, -1.4716e-01,  1.5993e-01, -2.1357e-01,\n",
       "          2.8887e-01, -1.2604e-01, -7.7528e-02, -9.0030e-02, -3.7376e-03,\n",
       "          1.0720e-01,  3.0606e-02, -3.5324e-03, -2.0448e-01,  1.4680e-01,\n",
       "         -3.4882e-02, -1.0665e-01,  1.2057e-02, -1.3353e-02,  1.0711e-01,\n",
       "          8.8180e-02, -2.0940e-03, -1.2773e-01, -8.9343e-02,  1.0484e-01,\n",
       "          2.3825e-02, -9.3665e-02, -2.2037e-01,  6.8377e-02,  9.0310e-03,\n",
       "          7.6072e-02, -2.2031e-02,  3.7002e-02,  2.6862e-02, -2.2119e-01,\n",
       "         -8.0299e-02,  6.3590e-02, -6.9066e-02,  1.4395e-02, -4.4914e-02,\n",
       "          7.7778e-02,  3.8778e-02, -1.7181e-01, -6.0108e-02, -3.5398e-02,\n",
       "          2.2103e-01,  4.0793e-02,  1.5110e-01, -2.2807e-01,  5.4516e-02,\n",
       "          4.0307e-01, -3.1999e-02, -5.2646e-02, -1.2886e-01, -1.0468e-01,\n",
       "          5.5570e-02, -4.0301e-02, -7.3852e-02, -1.4311e-02, -4.0545e-02,\n",
       "         -8.6112e-02, -3.6244e-02, -5.4362e-02, -4.4935e-02,  1.9113e-01,\n",
       "          5.5830e-02, -8.5965e-03, -2.1372e-01, -3.5327e-01, -1.5151e-01,\n",
       "          1.8747e-02,  2.2844e-01,  2.4797e-01, -8.2298e-02, -4.2324e-02,\n",
       "          5.1168e-02,  3.2533e-01,  9.3319e-02,  6.2971e-02, -2.8695e-01,\n",
       "         -1.9898e-02,  1.6351e-01, -1.0179e-01, -4.2131e-02,  9.1442e-02,\n",
       "          9.2500e-03, -1.1454e-01,  4.5913e-02, -2.9634e-01,  1.2875e-01,\n",
       "         -6.8871e-02, -2.2218e-01,  1.5565e-02,  1.4979e-01,  1.0188e-01,\n",
       "         -1.1626e-02, -1.2291e-01, -1.7825e-01, -2.8429e-02, -7.8027e-02,\n",
       "         -4.6080e-02,  1.2210e-01,  3.7010e-02, -1.0867e-01, -4.5216e-03,\n",
       "          6.4527e-02, -7.9008e-02, -6.5508e-02,  8.3449e-02, -1.6432e-01,\n",
       "          1.1451e-01, -1.7484e-01,  1.4081e-02, -3.1912e-02, -9.7364e-02,\n",
       "          2.0718e-01,  1.1974e-01, -8.2325e-02,  6.4259e-03, -3.3141e-02,\n",
       "         -6.1438e-02,  2.3983e-02, -3.2914e-02,  6.1812e-02,  6.2340e-02,\n",
       "          9.2117e-02,  8.0326e-03, -9.2124e-02,  1.1878e-01,  1.1687e-01,\n",
       "          7.4737e-02,  1.7057e-01,  4.3015e-02,  9.4886e-02,  1.0688e-01,\n",
       "          2.4408e-01,  1.2037e-01, -9.9698e-02,  1.8513e-01, -2.1800e-02,\n",
       "         -9.4436e-02,  7.7614e-02, -6.4549e-02,  2.3555e-01, -2.1412e-01,\n",
       "          1.6764e-01,  2.2875e-01,  8.7989e-03, -9.6676e-02, -6.1373e-02,\n",
       "         -2.1836e-02, -3.5840e-02,  1.6550e-01,  5.8282e-02, -9.9026e-02,\n",
       "          5.2696e-02,  8.6653e-02,  1.0248e-01,  3.8106e-03,  1.4792e-01,\n",
       "         -1.4293e-01,  5.6525e-02,  1.4934e-01, -9.8117e-02, -1.0121e-01,\n",
       "         -3.1885e-01, -1.2244e-01, -8.5570e-02,  9.1570e-02,  1.9804e-01,\n",
       "          4.7690e-02, -5.6648e-02, -9.1541e-03,  8.8989e-05,  5.4733e-02,\n",
       "         -1.2317e-01,  2.8750e-02, -1.3568e-01,  1.0916e-01, -2.7061e-01,\n",
       "         -1.0629e-01,  1.0810e-01,  3.3264e-02,  1.3156e-01, -1.9574e-01,\n",
       "          6.5248e-02,  1.3896e-01, -2.4210e-02, -6.1764e-02,  1.7800e-02,\n",
       "         -2.2708e-01, -3.7458e-02, -2.0157e-01,  4.9938e-02, -1.0165e-01,\n",
       "          1.5488e-01,  3.1902e-02,  7.9320e-02,  8.1731e-02,  1.1739e-01,\n",
       "          3.7342e-02,  2.5183e-01, -2.0013e-02, -2.5922e-02,  1.8994e-01,\n",
       "          5.7014e-02,  1.2591e-01, -3.3680e-02,  4.7639e-02, -1.8211e-01,\n",
       "         -1.2308e-01, -1.2009e-02,  1.9676e-01, -1.3294e-01, -2.1767e-01,\n",
       "          6.6980e-02, -2.5837e-02, -8.1987e-02, -7.7908e-02, -8.4370e-02,\n",
       "          8.3230e-02,  8.0932e-03, -1.1156e-01, -8.2674e-02, -1.2652e-01,\n",
       "         -5.9165e-02, -8.4191e-03, -9.5730e-03,  2.4593e-01, -4.4441e-02,\n",
       "          6.4487e-02,  1.5621e-01,  4.5186e-02, -3.1937e-03, -8.0194e-02,\n",
       "          1.6483e-01, -8.4714e-02, -6.5093e-03,  2.1039e-01,  8.1530e-02,\n",
       "         -9.5290e-02, -1.0459e-01, -5.6670e-02,  4.9779e-02, -1.5031e-01,\n",
       "          2.0671e-01,  4.5562e-02, -5.0857e-02,  7.2514e-02,  2.0370e-02,\n",
       "         -1.5000e-02,  8.5791e-02,  8.0964e-02,  5.6683e-02, -1.1588e-01,\n",
       "         -1.9894e-01, -3.9301e-02,  5.5085e-02,  1.4421e-01,  2.6269e-02,\n",
       "         -2.0137e-01,  6.4624e-02,  3.2710e-02,  1.5808e-01,  9.6461e-02,\n",
       "         -1.9221e-03,  4.1879e-02, -3.0835e-02, -1.4358e-01, -4.1478e-02,\n",
       "          1.6383e-01,  1.9469e-01,  6.9942e-02, -3.1289e-02,  5.9899e-02,\n",
       "         -1.4225e-01,  2.0471e-01, -6.8610e-02, -7.4580e-03,  2.7324e-01,\n",
       "         -1.9989e-01, -1.4467e-01, -3.3145e-01,  6.6554e-03, -5.9975e-02,\n",
       "          1.3844e-02,  9.0176e-02, -4.8052e-02,  2.6675e-01, -6.8163e-02,\n",
       "          4.5815e-02, -8.9561e-02, -8.1276e-02,  6.5407e-02, -1.4733e-01,\n",
       "          1.7196e-01, -1.0978e-01, -7.5249e-02, -1.8168e-01, -9.1237e-02,\n",
       "         -1.6084e-01, -7.6697e-02, -5.1686e-02,  1.2863e-01, -4.6578e-02,\n",
       "          1.2610e-01,  1.3656e-01, -4.7904e-02,  1.4487e-02,  1.1827e-01,\n",
       "          3.7647e-02, -6.7651e-02, -3.3152e-02, -1.6582e-02,  2.3107e-01,\n",
       "         -2.3420e-02,  1.0800e-01, -9.3380e-02,  1.6227e-01, -1.1909e-01,\n",
       "          1.2744e-01, -1.4296e-01,  9.7934e-02, -2.2750e-01,  1.4999e-01,\n",
       "         -1.0132e-01, -2.8187e-01, -1.0820e-01,  8.6087e-02, -3.7300e-02,\n",
       "         -8.4353e-02, -1.4519e-01,  2.1153e-01,  3.9248e-03,  8.9334e-02,\n",
       "          1.0934e-01, -4.1655e-02, -5.2318e-02, -2.7252e-01,  1.3986e-01,\n",
       "          7.4561e-03,  1.4774e-01,  1.2146e-01, -4.7115e-02, -1.3679e-01,\n",
       "          1.2778e-01,  6.8241e-02, -9.3501e-02,  1.1279e-01, -1.2951e-01,\n",
       "         -6.8587e-02, -8.9779e-02,  1.7672e-01,  1.4715e-01, -1.7105e-01,\n",
       "         -3.7738e-03,  1.0300e-01, -1.9733e-01, -1.5164e-01,  1.0093e-01,\n",
       "          9.1934e-02, -1.8665e-02, -6.2494e-02,  1.3560e-01, -1.3334e-01,\n",
       "          1.5803e-01, -5.6651e-02,  1.8629e-01, -1.1901e-01,  1.1399e-01,\n",
       "          3.0465e-02,  2.4028e-02, -1.2062e-01, -1.5195e-02,  5.1004e-02,\n",
       "         -1.5738e-01,  7.9901e-02, -2.1803e-01,  7.1776e-02, -3.7404e-02,\n",
       "         -3.0148e-01, -8.9080e-02,  1.5335e-01, -2.8322e-01, -9.6688e-02,\n",
       "          3.3668e-02,  4.5811e-03, -4.2585e-02, -1.1683e-02,  1.7150e-01,\n",
       "          2.1995e-01, -3.9893e-02, -2.4181e-02,  1.2752e-01, -2.7590e-01,\n",
       "         -2.3314e-01,  1.2471e-01, -5.0694e-02, -5.8867e-02, -6.4217e-02,\n",
       "         -1.8542e-01,  2.5694e-02,  8.0692e-02,  6.3986e-02, -1.7553e-01,\n",
       "          2.5718e-01,  9.6154e-02,  1.3346e-01,  1.0287e-01,  2.0176e-01,\n",
       "          1.3794e-01,  1.6850e-02,  2.1437e-02, -4.4000e-02,  6.1431e-02,\n",
       "         -1.4756e-01,  4.1605e-02,  3.1033e-02,  1.5018e-02,  1.1396e-02,\n",
       "         -1.7366e-03, -1.0827e-01, -2.0651e-03, -7.6129e-02,  2.0254e-01,\n",
       "          4.4048e-02, -1.0034e-01, -2.4310e-01, -8.8877e-03,  1.7000e-02,\n",
       "         -5.2983e-02, -1.5684e-01,  6.3094e-02,  1.8192e-02, -1.5621e-03,\n",
       "         -3.3343e-02, -1.1348e-01,  6.4691e-02, -1.6485e-01,  8.5106e-02,\n",
       "         -1.2114e-01, -1.0387e-01, -1.4729e-01,  9.1656e-03, -6.3657e-02,\n",
       "         -3.7153e-02, -1.2904e-01, -4.2606e-02, -6.0141e-02,  7.5812e-02,\n",
       "         -1.3157e-01,  1.4436e-01, -1.0464e-01, -1.1416e-01,  7.9633e-02,\n",
       "          1.7953e-01,  1.2322e-01,  2.9241e-02, -5.2600e-03,  3.1857e-02,\n",
       "         -5.4638e-02,  4.0499e-02, -1.9993e-01, -1.6358e-01, -1.9711e-01,\n",
       "         -4.8302e-02,  2.2780e-01,  4.5039e-02,  4.7857e-02,  4.3121e-02,\n",
       "         -1.5235e-01, -3.1036e-02, -1.6080e-02,  2.4168e-01,  1.4036e-02,\n",
       "         -1.5234e-02, -1.0252e-01, -8.5358e-02,  6.2481e-02,  9.6270e-02,\n",
       "          9.6394e-02, -7.8013e-02, -1.2592e-01, -9.3685e-02, -4.6496e-02,\n",
       "         -4.9995e-02, -6.2882e-02, -1.1116e-01, -2.5991e-01,  1.5717e-02,\n",
       "         -1.3967e-01,  1.6700e-01, -1.2823e-01, -5.1507e-02, -1.0037e-01,\n",
       "          3.8639e-02, -2.0298e-01,  1.2107e-01, -2.2835e-03, -1.8464e-01,\n",
       "          1.7442e-01, -1.5782e-01,  2.6361e-02,  2.2225e-02,  3.8786e-02,\n",
       "         -1.1433e-01,  9.5224e-02, -1.3414e-01,  7.7350e-02,  2.3523e-02,\n",
       "          4.9878e-02,  3.4112e-01,  6.8863e-02,  8.3088e-02, -3.9586e-02,\n",
       "          1.5086e-01, -1.0773e-01, -4.7396e-02, -1.1670e-01,  4.1485e-02,\n",
       "          1.1025e-01,  3.9003e-02, -3.7623e-02, -5.9998e-02,  1.7394e-01,\n",
       "         -8.3686e-02, -3.1000e-02,  1.1241e-01, -4.0111e-02, -9.3402e-02,\n",
       "         -3.8386e-02,  7.1734e-03,  1.6798e-01, -2.0445e-01, -8.6230e-02,\n",
       "         -1.9180e-02,  2.7865e-01, -5.3786e-02,  5.9780e-03,  1.0580e-01,\n",
       "          5.3805e-02, -2.5893e-01,  1.6558e-01,  1.0946e-01,  2.1189e-01,\n",
       "         -2.4153e-02, -4.5226e-02,  1.2257e-02, -2.1819e-02, -4.0649e-02,\n",
       "         -2.5746e-02, -9.5953e-02, -1.0741e-01,  5.6460e-02, -2.6453e-01,\n",
       "         -2.3815e-02, -6.7014e-02,  3.8681e-02, -6.5795e-02, -4.0038e-02,\n",
       "         -4.5136e-02, -1.0784e-01,  2.8146e-02,  5.2294e-02,  2.2442e-01,\n",
       "          2.0958e-01, -5.7937e-02, -7.8862e-02,  1.2087e-01, -3.3995e-02,\n",
       "          2.0321e-01, -6.6053e-02,  9.6477e-02, -3.8712e-02, -2.6210e-01,\n",
       "         -1.4872e-01, -3.1711e-02, -6.2195e-03,  2.3407e-02, -4.7610e-02,\n",
       "          5.3888e-02,  3.8343e-02,  1.4873e-02,  8.0151e-02, -1.0565e-01,\n",
       "         -3.3934e-02, -1.3229e-01,  2.0048e-01, -1.5456e-01, -1.2387e-01,\n",
       "          6.4634e-02,  7.4124e-02,  5.0192e-02, -5.6701e-02,  1.1958e-01,\n",
       "         -1.8150e-01,  1.3747e-01, -7.5286e-03,  8.6209e-02, -2.2532e-01,\n",
       "          8.1097e-03, -2.5569e-02, -4.6149e-02,  1.2853e-03, -5.8531e-02,\n",
       "          2.4016e-01,  5.5544e-02,  3.4893e-02, -6.5497e-03,  8.8703e-02,\n",
       "         -6.9136e-02, -1.1741e-01, -6.6744e-02,  1.1593e-01, -1.4698e-01,\n",
       "          4.2873e-02,  1.4679e-01, -3.5813e-02, -9.5585e-02, -1.2937e-02,\n",
       "          1.3863e-01,  1.5952e-01, -3.3239e-03, -7.4544e-02,  1.3293e-02,\n",
       "          7.5738e-02, -8.0271e-02, -2.2324e-01, -1.1320e-01,  4.7056e-03,\n",
       "         -3.6624e-02,  3.0547e-02,  2.1874e-01, -1.5836e-01, -7.8673e-02,\n",
       "         -6.6069e-02,  3.1266e-03, -2.2481e-01,  3.8885e-02, -2.3641e-02,\n",
       "          3.2680e-02,  9.1670e-02,  1.1407e-01, -1.6080e-02, -1.3548e-01,\n",
       "          1.1714e-01, -4.8438e-02,  2.3951e-01, -7.7091e-03,  6.3803e-02,\n",
       "         -8.1406e-02, -2.4688e-01, -1.6671e-01,  5.0732e-02,  1.0052e-01,\n",
       "         -5.6219e-02, -7.0956e-03, -5.2804e-02, -2.4068e-01, -1.8139e-01,\n",
       "          9.3612e-02, -1.2181e-01,  5.0559e-02, -1.7959e-01, -6.2926e-02,\n",
       "          2.0015e-01, -1.9401e-02,  1.1123e-01,  1.1058e-01,  1.8178e-02,\n",
       "         -1.9270e-02,  1.3900e-01, -2.6745e-02]], grad_fn=<TanhBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained('ckiplab/albert-base-chinese-ner',num_labels=22)\n",
    "# a = model(encoded['input_ids'], encoded['attention_mask'], labels=torch.tensor([[14,15,15,3,3,2,2,2,2,2]]))\n",
    "model(encoded['input_ids'], encoded['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel,AutoModel\n",
    "\n",
    "START_TAG = '<START>'\n",
    "STOP_TAG = '<END>'\n",
    "PAD_TAG = '<PAD>'\n",
    "\n",
    "class BERT_NER(nn.Module):\n",
    "    \"\"\"\n",
    "        fine-tune pretrained BERT-NER model on our dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparams, tag2idx):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = hparams['batch_size']\n",
    "        self.seq_length = hparams['seq_length']\n",
    "\n",
    "        self.device = hparams['device']\n",
    "\n",
    "        self.tag2idx = tag2idx\n",
    "        self.tagset_size = len(tag2idx)\n",
    "        self.idx2tag = {v:k for k,v in tag2idx.items()}\n",
    "\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim // 2, bidirectional=True)\n",
    "\n",
    "        self.bert = AutoModel.from_pretrained(\n",
    "            'ckiplab/albert-base-chinese-ner',\n",
    "            # output hidden embedding of each transformer layer\n",
    "        )\n",
    "\n",
    "        self.hidden2tag = nn.Linear(self.embedding_dim, self.tagset_size)\n",
    "        self.Softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def _bert_encode(token, attn_mask):\n",
    "        output = self.bert(token, token_mask)\n",
    "        embedding = output['last_hidden_state']\n",
    "        tag_prob = self.Softmax(self.hidden2tag(embedding))\n",
    "\n",
    "    \n",
    "    def forward(x):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of AlbertModel were not initialized from the model checkpoint at ckiplab/albert-base-chinese and are newly initialized: ['albert.pooler.weight', 'albert.pooler.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained('ckiplab/albert-base-chinese', output_hidden_states=True)\n",
    "# a = model(encoded['input_ids'], encoded['attention_mask'], labels=torch.tensor([[14,15,15,3,3,2,2,2,2,2]]))\n",
    "a = model(encoded['input_ids'], encoded['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "a['hidden_states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}