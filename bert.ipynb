{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Base BERT with second-to-last pooling for word representation, then apply CRF to calculate sentence score, and minimize the negative log likelihood to train"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.bert_model import BERT_CRF\n",
    "from utils.utils import prepare, predict, train, evaluate\n",
    "\n",
    "hparams = {\n",
    "    # 'path':'data/data_test_en.txt', \n",
    "    'path':'data/data.txt',\n",
    "    'epochs': 100,\n",
    "    'batch_size': 32,\n",
    "    'embedding_dim': 768,\n",
    "    'device':'cuda:1',\n",
    "    'bert':'bert-base-chinese',\n",
    "    'seq_length':20,\n",
    "    'learning_rate': 3e-5,\n",
    "    'save_path':'model_params/bert_base_model'\n",
    "}\n",
    "\n",
    "attr_dict, loaders = prepare(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BERT_CRF(hparams,attr_dict['tag2idx']).to(hparams['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = train(hparams, bert_model, loaders, lr=hparams['learning_rate'], schedule=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(bert_model, loaders[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bert_model.state_dict(),hparams['save_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_model.load_state_dict(torch.load(hparams['save_path']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(['中国人民大学第三十五届一二九合唱音乐节如期举行，信息学院分团委文化部将组织信院全体同学参加','张配天获得最佳论文奖'], bert_model, attr_dict['tokenizer'], 50)"
   ]
  },
  {
   "source": [
    "### Built-in BertForTokenClassification with labels input, normalize the output logits for classification, use the output loss for training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.bert_model import BERT_BASE\n",
    "from utils.utils import prepare, predict, train, evaluate\n",
    "\n",
    "hparams = {\n",
    "    # 'path':'data/data_test_en.txt', \n",
    "    'path':'data/data.txt',\n",
    "    'epochs': 100,\n",
    "    'batch_size': 40,\n",
    "    'embedding_dim': 768,\n",
    "    'device': 'cuda:1',\n",
    "    'bert': 'bert-base-chinese',\n",
    "    'seq_length': 256,\n",
    "    'learning_rate': 3e-5\n",
    "}\n",
    "\n",
    "attr_dict, loaders = prepare(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BERT_BASE(hparams, attr_dict['tag2idx']).to(hparams['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = train(hparams, bert_model, loaders, lr=hparams['learning_rate'], schedule=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(bert_model, loaders[1])"
   ]
  },
  {
   "source": [
    "### NER pretrained BERT with last hidden state pooling, then directly map the 768 dimensional hidden states to the tagset space, minimize the negative log likelihood for classification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.bert_model import BERT_NER\n",
    "from utils.utils import prepare, predict, train, evaluate\n",
    "\n",
    "hparams = {\n",
    "    # 'path':'data/data_test_en.txt', \n",
    "    'path':'data/data.txt',\n",
    "    'epochs': 100,\n",
    "    'batch_size': 32,\n",
    "    'embedding_dim': 768,\n",
    "    'device':'cuda:1',\n",
    "    # 'bert':'dslim/bert-base-NER', \n",
    "    'bert': 'ckiplab/bert-base-chinese-ner',\n",
    "    'seq_length': 256,\n",
    "    'learning_rate': 3e-5\n",
    "}\n",
    "attr_dict, loaders = prepare(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BERT_NER(hparams, attr_dict['tag2idx']).to(hparams['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = train(hparams, bert_model, loaders, lr=hparams['learning_rate'], schedule=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(bert_model, loaders[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}