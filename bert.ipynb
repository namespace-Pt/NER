{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitnnconda048fc75fe4ee43f1aa97608c8881ebba",
   "display_name": "Python 3.8.5 64-bit ('nn': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Base BERT with second-to-last pooling for word representation, then apply CRF to calculate sentence score, and minimize the negative log likelihood to train"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.bert_model import BERT_CRF\n",
    "from utils.utils import prepare, predict, train, evaluate\n",
    "\n",
    "hparams = {\n",
    "    # 'path':'data/data_test_en.txt', \n",
    "    'path':'data/data.txt',\n",
    "    'epochs': 100,\n",
    "    'batch_size': 32,\n",
    "    'embedding_dim': 768,\n",
    "    'device':'cuda:0',\n",
    "    'bert':'bert-base-chinese',\n",
    "    'seq_length':256,\n",
    "    'learning_rate': 3e-5,\n",
    "    'save_path':'model_params/bert_base_model'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_dict, loaders = prepare(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BERT_CRF(hparams,attr_dict['tag2idx']).to(hparams['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch 1 , step 27 , loss: 316.9347: : 28it [00:20,  1.34it/s]\n",
      "{'macro_f1': 0.1628, 'weighted_f1': 0.8052}\n",
      "epoch 2 , step 27 , loss: 152.0206: : 28it [00:20,  1.33it/s]\n",
      "{'macro_f1': 0.3235, 'weighted_f1': 0.8574}\n",
      "epoch 3 , step 27 , loss: 109.7785: : 28it [00:20,  1.36it/s]\n",
      "{'macro_f1': 0.4243, 'weighted_f1': 0.8813}\n",
      "epoch 4 , step 27 , loss: 85.2446: : 28it [00:20,  1.37it/s]\n",
      "{'macro_f1': 0.447, 'weighted_f1': 0.8882}\n",
      "epoch 5 , step 27 , loss: 72.6541: : 28it [00:20,  1.35it/s]\n",
      "{'macro_f1': 0.4732, 'weighted_f1': 0.8947}\n",
      "epoch 6 , step 27 , loss: 59.6001: : 28it [00:20,  1.38it/s]\n",
      "{'macro_f1': 0.4989, 'weighted_f1': 0.8986}\n",
      "epoch 7 , step 27 , loss: 50.1959: : 28it [00:20,  1.37it/s]\n",
      "{'macro_f1': 0.522, 'weighted_f1': 0.9015}\n",
      "epoch 8 , step 27 , loss: 43.1029: : 28it [00:20,  1.36it/s]\n",
      "{'macro_f1': 0.5324, 'weighted_f1': 0.9028}\n",
      "epoch 9 , step 27 , loss: 38.2612: : 28it [00:20,  1.36it/s]\n",
      "{'macro_f1': 0.5398, 'weighted_f1': 0.9053}\n",
      "epoch 10 , step 27 , loss: 34.9644: : 28it [00:20,  1.39it/s]\n",
      "{'macro_f1': 0.5492, 'weighted_f1': 0.9073}\n",
      "epoch 11 , step 27 , loss: 32.6139: : 28it [00:20,  1.39it/s]\n",
      "{'macro_f1': 0.548, 'weighted_f1': 0.9062}\n",
      "epoch 12 , step 27 , loss: 27.8805: : 28it [00:20,  1.38it/s]\n",
      "{'macro_f1': 0.5635, 'weighted_f1': 0.906}\n",
      "epoch 13 , step 27 , loss: 24.0054: : 28it [00:20,  1.39it/s]\n",
      "{'macro_f1': 0.5598, 'weighted_f1': 0.9053}\n",
      "epoch 14 , step 27 , loss: 22.2095: : 28it [00:20,  1.38it/s]\n",
      "{'macro_f1': 0.5739, 'weighted_f1': 0.9072}\n",
      "epoch 15 , step 27 , loss: 20.1663: : 28it [00:20,  1.37it/s]\n",
      "{'macro_f1': 0.5678, 'weighted_f1': 0.9069}\n",
      "epoch 16 , step 27 , loss: 18.6769: : 28it [00:20,  1.39it/s]\n",
      "{'macro_f1': 0.5657, 'weighted_f1': 0.9052}\n",
      "epoch 17 , step 27 , loss: 17.7142: : 28it [00:20,  1.37it/s]\n",
      "{'macro_f1': 0.5705, 'weighted_f1': 0.9062}\n",
      "epoch 18 , step 27 , loss: 16.1936: : 28it [00:20,  1.38it/s]\n",
      "{'macro_f1': 0.5717, 'weighted_f1': 0.9056}\n",
      "epoch 19 , step 27 , loss: 15.7922: : 28it [00:20,  1.38it/s]\n",
      "{'macro_f1': 0.5673, 'weighted_f1': 0.9065}\n",
      "epoch 20 , step 27 , loss: 14.3056: : 28it [00:20,  1.37it/s]\n",
      "{'macro_f1': 0.5577, 'weighted_f1': 0.9047}\n",
      "epoch 21 , step 27 , loss: 12.7791: : 28it [00:20,  1.37it/s]\n",
      "{'macro_f1': 0.5681, 'weighted_f1': 0.9069}\n",
      "epoch 22 , step 27 , loss: 10.9758: : 28it [00:20,  1.39it/s]\n",
      "{'macro_f1': 0.5699, 'weighted_f1': 0.9065}\n",
      "epoch 23 , step 27 , loss: 10.2957: : 28it [00:20,  1.39it/s]\n",
      "{'macro_f1': 0.5656, 'weighted_f1': 0.9063}\n",
      "epoch 24 , step 27 , loss: 9.8752: : 28it [00:20,  1.38it/s]\n",
      "{'macro_f1': 0.5693, 'weighted_f1': 0.9052}\n",
      "epoch 25 , step 27 , loss: 8.8356: : 28it [00:20,  1.38it/s]\n",
      "{'macro_f1': 0.5695, 'weighted_f1': 0.9056}\n",
      "epoch 26 , step 27 , loss: 8.7462: : 28it [00:20,  1.36it/s]\n",
      "{'macro_f1': 0.5633, 'weighted_f1': 0.9059}\n",
      "epoch 27 , step 27 , loss: 8.4310: : 28it [00:20,  1.37it/s]\n",
      "{'macro_f1': 0.57, 'weighted_f1': 0.9061}\n",
      "epoch 28 , step 27 , loss: 7.2251: : 28it [00:20,  1.37it/s]\n",
      "{'macro_f1': 0.5612, 'weighted_f1': 0.9042}\n",
      "epoch 29 , step 27 , loss: 6.2266: : 28it [00:20,  1.38it/s]\n",
      "{'macro_f1': 0.5637, 'weighted_f1': 0.9041}\n",
      "epoch 30 , step 27 , loss: 5.5524: : 28it [00:20,  1.38it/s]\n",
      "{'macro_f1': 0.5706, 'weighted_f1': 0.9052}\n",
      "epoch 31 , step 27 , loss: 4.9024: : 28it [00:20,  1.38it/s]\n",
      "{'macro_f1': 0.5632, 'weighted_f1': 0.9045}\n",
      "epoch 32 , step 27 , loss: 4.4001: : 28it [00:20,  1.39it/s]\n",
      "{'macro_f1': 0.5592, 'weighted_f1': 0.9043}\n",
      "epoch 33 , step 27 , loss: 3.9219: : 28it [00:20,  1.35it/s]\n",
      "{'macro_f1': 0.5666, 'weighted_f1': 0.9049}\n",
      "epoch 34 , step 27 , loss: 3.8952: : 28it [00:20,  1.36it/s]\n",
      "{'macro_f1': 0.5697, 'weighted_f1': 0.9044}\n",
      "epoch 35 , step 27 , loss: 3.2664: : 28it [00:20,  1.37it/s]\n",
      "{'macro_f1': 0.5752, 'weighted_f1': 0.9056}\n",
      "epoch 36 , step 27 , loss: 3.4732: : 28it [00:20,  1.40it/s]\n",
      "{'macro_f1': 0.5698, 'weighted_f1': 0.9045}\n",
      "epoch 37 , step 27 , loss: 3.3763: : 28it [00:20,  1.39it/s]\n",
      "{'macro_f1': 0.5619, 'weighted_f1': 0.9051}\n",
      "epoch 38 , step 27 , loss: 2.7842: : 28it [00:20,  1.37it/s]\n",
      "{'macro_f1': 0.5664, 'weighted_f1': 0.9047}\n",
      "epoch 39 , step 27 , loss: 2.4817: : 28it [00:20,  1.38it/s]\n",
      "{'macro_f1': 0.5651, 'weighted_f1': 0.9047}\n",
      "epoch 40 , step 27 , loss: 2.1572: : 28it [00:20,  1.39it/s]\n",
      "{'macro_f1': 0.5643, 'weighted_f1': 0.9048}\n",
      "epoch 41 , step 27 , loss: 2.1127: : 28it [00:20,  1.39it/s]\n",
      "{'macro_f1': 0.5743, 'weighted_f1': 0.9049}\n",
      "epoch 42 , step 27 , loss: 2.0615: : 28it [00:20,  1.38it/s]\n",
      "{'macro_f1': 0.5744, 'weighted_f1': 0.9046}\n",
      "epoch 43 , step 27 , loss: 2.0736: : 28it [00:20,  1.37it/s]\n",
      "{'macro_f1': 0.5716, 'weighted_f1': 0.9049}\n",
      "epoch 44 , step 27 , loss: 1.9197: : 28it [00:20,  1.39it/s]\n",
      "{'macro_f1': 0.5677, 'weighted_f1': 0.9052}\n",
      "epoch 45 , step 27 , loss: 1.7932: : 28it [00:20,  1.35it/s]\n",
      "{'macro_f1': 0.5736, 'weighted_f1': 0.9063}\n",
      "epoch 46 , step 27 , loss: 1.6437: : 28it [00:20,  1.38it/s]\n",
      "{'macro_f1': 0.5626, 'weighted_f1': 0.9046}\n",
      "epoch 47 , step 27 , loss: 1.4228: : 28it [00:20,  1.33it/s]\n",
      "{'macro_f1': 0.5661, 'weighted_f1': 0.9051}\n",
      "epoch 48 , step 27 , loss: 1.2655: : 28it [00:20,  1.34it/s]\n",
      "{'macro_f1': 0.5636, 'weighted_f1': 0.9049}\n",
      "epoch 49 , step 27 , loss: 1.2190: : 28it [00:20,  1.38it/s]\n",
      "{'macro_f1': 0.5619, 'weighted_f1': 0.9053}\n",
      "epoch 50 , step 27 , loss: 1.2067: : 28it [00:20,  1.37it/s]\n",
      "{'macro_f1': 0.5647, 'weighted_f1': 0.9053}\n",
      "epoch 51 , step 27 , loss: 1.1190: : 28it [00:20,  1.38it/s]\n",
      "{'macro_f1': 0.5637, 'weighted_f1': 0.9047}\n",
      "epoch 52 , step 27 , loss: 1.0974: : 28it [00:20,  1.37it/s]\n",
      "{'macro_f1': 0.5646, 'weighted_f1': 0.9053}\n",
      "epoch 53 , step 27 , loss: 0.9940: : 28it [00:20,  1.40it/s]\n",
      "{'macro_f1': 0.5651, 'weighted_f1': 0.9047}\n",
      "epoch 54 , step 27 , loss: 0.9016: : 28it [00:20,  1.39it/s]\n",
      "{'macro_f1': 0.5621, 'weighted_f1': 0.9047}\n",
      "epoch 55 , step 27 , loss: 0.9317: : 28it [00:20,  1.38it/s]\n",
      "{'macro_f1': 0.5668, 'weighted_f1': 0.9058}\n",
      "epoch 56 , step 27 , loss: 0.9371: : 28it [00:20,  1.37it/s]\n",
      "{'macro_f1': 0.5622, 'weighted_f1': 0.9048}\n",
      "epoch 57 , step 27 , loss: 0.9102: : 28it [00:20,  1.38it/s]\n",
      "{'macro_f1': 0.5616, 'weighted_f1': 0.9049}\n",
      "epoch 58 , step 27 , loss: 0.8749: : 28it [00:20,  1.38it/s]\n",
      "{'macro_f1': 0.569, 'weighted_f1': 0.905}\n",
      "epoch 59 , step 27 , loss: 0.7789: : 28it [00:20,  1.36it/s]\n",
      "{'macro_f1': 0.5614, 'weighted_f1': 0.9056}\n",
      "epoch 60 , step 27 , loss: 0.7507: : 28it [00:20,  1.37it/s]\n",
      "{'macro_f1': 0.5631, 'weighted_f1': 0.9051}\n",
      "epoch 61 , step 27 , loss: 0.7541: : 28it [00:20,  1.37it/s]\n",
      "{'macro_f1': 0.564, 'weighted_f1': 0.9051}\n",
      "epoch 62 , step 27 , loss: 0.7174: : 28it [00:20,  1.38it/s]\n",
      "{'macro_f1': 0.5663, 'weighted_f1': 0.9048}\n",
      "epoch 63 , step 27 , loss: 0.7198: : 28it [00:20,  1.37it/s]\n",
      "{'macro_f1': 0.5623, 'weighted_f1': 0.905}\n",
      "epoch 64 , step 27 , loss: 0.6317: : 28it [00:20,  1.38it/s]\n",
      "{'macro_f1': 0.562, 'weighted_f1': 0.905}\n",
      "epoch 65 , step 27 , loss: 0.6311: : 28it [00:20,  1.38it/s]\n",
      "{'macro_f1': 0.5629, 'weighted_f1': 0.9052}\n",
      "epoch 66 , step 27 , loss: 0.6085: : 28it [00:20,  1.39it/s]\n",
      "{'macro_f1': 0.5604, 'weighted_f1': 0.905}\n",
      "epoch 67 , step 27 , loss: 0.5741: : 28it [00:20,  1.38it/s]\n",
      "{'macro_f1': 0.5646, 'weighted_f1': 0.9052}\n",
      "epoch 68 , step 27 , loss: 0.5544: : 28it [00:20,  1.37it/s]\n",
      "{'macro_f1': 0.5598, 'weighted_f1': 0.905}\n",
      "epoch 69 , step 27 , loss: 0.5899: : 28it [00:20,  1.35it/s]\n",
      "{'macro_f1': 0.5647, 'weighted_f1': 0.9052}\n",
      "epoch 70 , step 27 , loss: 0.5447: : 28it [00:20,  1.38it/s]\n",
      "{'macro_f1': 0.5606, 'weighted_f1': 0.9052}\n",
      "epoch 71 , step 27 , loss: 0.6218: : 28it [00:20,  1.37it/s]\n",
      "{'macro_f1': 0.5598, 'weighted_f1': 0.9044}\n",
      "epoch 72 , step 27 , loss: 0.7045: : 28it [00:20,  1.36it/s]\n",
      "{'macro_f1': 0.5712, 'weighted_f1': 0.9054}\n",
      "epoch 73 , step 27 , loss: 0.7981: : 28it [00:20,  1.35it/s]\n",
      "{'macro_f1': 0.5652, 'weighted_f1': 0.9052}\n",
      "epoch 74 , step 27 , loss: 0.7878: : 28it [00:20,  1.39it/s]\n",
      "{'macro_f1': 0.5615, 'weighted_f1': 0.9048}\n",
      "epoch 75 , step 27 , loss: 0.7593: : 28it [00:20,  1.37it/s]\n",
      "{'macro_f1': 0.5635, 'weighted_f1': 0.9043}\n",
      "epoch 76 , step 27 , loss: 0.6206: : 28it [00:20,  1.38it/s]\n",
      "{'macro_f1': 0.5651, 'weighted_f1': 0.9049}\n",
      "epoch 77 , step 27 , loss: 0.5541: : 28it [00:20,  1.36it/s]\n",
      "{'macro_f1': 0.5609, 'weighted_f1': 0.9047}\n",
      "epoch 78 , step 27 , loss: 0.5490: : 28it [00:20,  1.36it/s]\n",
      "{'macro_f1': 0.5601, 'weighted_f1': 0.9046}\n",
      "epoch 79 , step 27 , loss: 0.5194: : 28it [00:20,  1.38it/s]\n",
      "{'macro_f1': 0.5598, 'weighted_f1': 0.9044}\n",
      "epoch 80 , step 27 , loss: 0.5081: : 28it [00:20,  1.38it/s]\n",
      "{'macro_f1': 0.5599, 'weighted_f1': 0.9046}\n",
      "epoch 81 , step 27 , loss: 0.5074: : 28it [00:20,  1.39it/s]\n",
      "{'macro_f1': 0.5608, 'weighted_f1': 0.9048}\n",
      "epoch 82 , step 27 , loss: 0.4862: : 28it [00:20,  1.39it/s]\n",
      "{'macro_f1': 0.5615, 'weighted_f1': 0.9048}\n",
      "epoch 83 , step 27 , loss: 0.4823: : 28it [00:20,  1.39it/s]\n",
      "{'macro_f1': 0.5617, 'weighted_f1': 0.905}\n",
      "epoch 84 , step 27 , loss: 0.4820: : 28it [00:20,  1.36it/s]\n",
      "{'macro_f1': 0.5604, 'weighted_f1': 0.9045}\n",
      "epoch 85 , step 27 , loss: 0.4808: : 28it [00:20,  1.37it/s]\n",
      "{'macro_f1': 0.5632, 'weighted_f1': 0.9048}\n",
      "epoch 86 , step 27 , loss: 0.4903: : 28it [00:20,  1.37it/s]\n",
      "{'macro_f1': 0.5615, 'weighted_f1': 0.9047}\n",
      "epoch 87 , step 27 , loss: 0.4656: : 28it [00:20,  1.37it/s]\n",
      "{'macro_f1': 0.5612, 'weighted_f1': 0.9047}\n",
      "epoch 88 , step 27 , loss: 0.4579: : 28it [00:20,  1.37it/s]\n",
      "{'macro_f1': 0.561, 'weighted_f1': 0.9047}\n",
      "epoch 89 , step 27 , loss: 0.4613: : 28it [00:20,  1.36it/s]\n",
      "{'macro_f1': 0.5612, 'weighted_f1': 0.9047}\n",
      "epoch 90 , step 27 , loss: 0.4579: : 28it [00:20,  1.39it/s]\n",
      "{'macro_f1': 0.5616, 'weighted_f1': 0.9047}\n",
      "epoch 91 , step 27 , loss: 0.4642: : 28it [00:20,  1.39it/s]\n",
      "{'macro_f1': 0.5614, 'weighted_f1': 0.9046}\n",
      "epoch 92 , step 27 , loss: 0.4496: : 28it [00:20,  1.37it/s]\n",
      "{'macro_f1': 0.5619, 'weighted_f1': 0.9046}\n",
      "epoch 93 , step 27 , loss: 0.4405: : 28it [00:20,  1.39it/s]\n",
      "{'macro_f1': 0.5618, 'weighted_f1': 0.9047}\n",
      "epoch 94 , step 27 , loss: 0.4389: : 28it [00:20,  1.38it/s]\n",
      "{'macro_f1': 0.5619, 'weighted_f1': 0.9046}\n",
      "epoch 95 , step 27 , loss: 0.4365: : 28it [00:20,  1.37it/s]\n",
      "{'macro_f1': 0.5606, 'weighted_f1': 0.9046}\n",
      "epoch 96 , step 27 , loss: 0.4366: : 28it [00:20,  1.38it/s]\n",
      "{'macro_f1': 0.5618, 'weighted_f1': 0.9047}\n",
      "epoch 97 , step 27 , loss: 0.4482: : 28it [00:20,  1.38it/s]\n",
      "{'macro_f1': 0.5614, 'weighted_f1': 0.9045}\n",
      "epoch 98 , step 27 , loss: 0.4325: : 28it [00:20,  1.38it/s]\n",
      "{'macro_f1': 0.5619, 'weighted_f1': 0.9047}\n",
      "epoch 99 , step 27 , loss: 0.4247: : 28it [00:20,  1.39it/s]\n",
      "{'macro_f1': 0.5613, 'weighted_f1': 0.9046}\n",
      "epoch 100 , step 27 , loss: 0.4220: : 28it [00:20,  1.37it/s]\n",
      "{'macro_f1': 0.5615, 'weighted_f1': 0.9047}\n"
     ]
    }
   ],
   "source": [
    "bert_model = train(hparams, bert_model, loaders, lr=hparams['learning_rate'], schedule=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           2       1.00      1.00      1.00    167874\n           3       0.93      0.91      0.92       143\n           4       0.98      0.97      0.97      1505\n           5       0.93      0.91      0.92       377\n           6       0.98      0.95      0.96      6027\n           7       0.96      0.95      0.95      1573\n           8       0.97      0.98      0.98      6749\n           9       0.95      0.94      0.95       294\n          10       0.98      0.98      0.98      1448\n          11       0.93      0.94      0.94       449\n          12       0.97      0.96      0.96      2432\n          13       0.95      0.96      0.96      1520\n          14       0.97      0.95      0.96      2850\n          15       0.93      0.93      0.93       771\n          16       0.96      0.95      0.95      5565\n          17       0.96      0.96      0.96      1574\n          18       0.97      0.96      0.96      3162\n          19       0.86      0.91      0.89       287\n          20       0.96      0.97      0.96      4100\n          21       0.98      0.98      0.98     43204\n\n    accuracy                           0.99    251904\n   macro avg       0.96      0.95      0.95    251904\nweighted avg       0.99      0.99      0.99    251904\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'macro_f1': 0.9544, 'weighted_f1': 0.99}"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "evaluate(bert_model, loaders[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bert_model.state_dict(),hparams['save_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# bert_model.load_state_dict(torch.load(hparams['save_path']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "************************\n[CLS] O\n中 B-department\n国 I-department\n人 I-department\n民 I-department\n大 I-department\n学 I-department\n第 B-department\n三 I-scholarship\n十 I-scholarship\n五 I-scholarship\n届 I-scholarship\n一 I-scholarship\n二 I-scholarship\n九 I-scholarship\n合 I-scholarship\n唱 I-scholarship\n音 I-scholarship\n乐 I-scholarship\n节 I-scholarship\n如 O\n期 I-scholarship\n举 O\n行 O\n， O\n信 B-department\n息 I-department\n学 I-department\n院 I-department\n分 I-department\n团 I-organization\n委 I-organization\n文 B-organization\n化 I-organization\n部 I-organization\n将 O\n组 O\n织 O\n信 O\n院 O\n全 O\n体 O\n同 O\n学 O\n参 O\n加 O\n[SEP] O\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n************************\n[CLS] O\n张 B-name\n配 I-name\n天 I-name\n获 O\n得 O\n最 O\n佳 I-award\n论 O\n文 O\n奖 O\n[SEP] O\n[PAD] O\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n[PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "predict(['中国人民大学第三十五届一二九合唱音乐节如期举行，信息学院分团委文化部将组织信院全体同学参加','张配天获得最佳论文奖'], bert_model, attr_dict['tokenizer'], 50)"
   ]
  },
  {
   "source": [
    "### Built-in BertForTokenClassification with labels input, normalize the output logits for classification, use the output loss for training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.bert_model import BERT_BASE\n",
    "from utils.utils import prepare, predict, train, evaluate\n",
    "\n",
    "hparams = {\n",
    "    # 'path':'data/data_test_en.txt', \n",
    "    'path':'data/data.txt',\n",
    "    'epochs': 100,\n",
    "    'batch_size': 40,\n",
    "    'embedding_dim': 768,\n",
    "    'device': 'cuda:0',\n",
    "    'bert': 'bert-base-chinese',\n",
    "    'seq_length': 256,\n",
    "    'learning_rate': 3e-5\n",
    "}\n",
    "\n",
    "attr_dict, loaders = prepare(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BERT_BASE(hparams, attr_dict['tag2idx']).to(hparams['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = train(hparams, bert_model, loaders, lr=hparams['learning_rate'], schedule=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(bert_model, loaders[1])"
   ]
  },
  {
   "source": [
    "### NER pretrained BERT with last hidden state pooling, then directly map the 768 dimensional hidden states to the tagset space, minimize the negative log likelihood for classification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.bert_model import BERT_NER\n",
    "from utils.utils import prepare, predict, train, evaluate\n",
    "\n",
    "hparams = {\n",
    "    # 'path':'data/data_test_en.txt', \n",
    "    'path':'data/data.txt',\n",
    "    'epochs': 100,\n",
    "    'batch_size': 32,\n",
    "    'embedding_dim': 768,\n",
    "    'device':'cuda:0',\n",
    "    # 'bert':'dslim/bert-base-NER', \n",
    "    'bert': 'ckiplab/bert-base-chinese-ner',\n",
    "    'seq_length': 256,\n",
    "    'learning_rate': 3e-5\n",
    "}\n",
    "attr_dict, loaders = prepare(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BERT_NER(hparams, attr_dict['tag2idx']).to(hparams['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch 1 , step 29 , loss: 82.0513: : 30it [00:03,  8.45it/s]\n",
      "{'macro_f1': 0.0295, 'weighted_f1': 0.2466}\n",
      "epoch 2 , step 29 , loss: 64.9663: : 30it [00:03,  9.09it/s]\n",
      "{'macro_f1': 0.0295, 'weighted_f1': 0.2466}\n",
      "epoch 3 , step 29 , loss: 63.0864: : 30it [00:03,  9.21it/s]\n",
      "{'macro_f1': 0.0295, 'weighted_f1': 0.2466}\n",
      "epoch 4 , step 29 , loss: 63.4545: : 30it [00:03,  9.11it/s]\n",
      "{'macro_f1': 0.0295, 'weighted_f1': 0.2466}\n",
      "epoch 5 , step 29 , loss: 64.0466: : 30it [00:03,  9.94it/s]\n",
      "{'macro_f1': 0.0295, 'weighted_f1': 0.2466}\n",
      "epoch 6 , step 29 , loss: 63.5238: : 30it [00:03,  9.86it/s]\n",
      "{'macro_f1': 0.0295, 'weighted_f1': 0.2466}\n",
      "epoch 7 , step 29 , loss: 63.3384: : 30it [00:03,  9.62it/s]\n",
      "{'macro_f1': 0.0295, 'weighted_f1': 0.2466}\n",
      "epoch 8 , step 29 , loss: 63.6640: : 30it [00:03,  8.89it/s]\n",
      "{'macro_f1': 0.0295, 'weighted_f1': 0.2466}\n",
      "epoch 9 , step 29 , loss: 63.2327: : 30it [00:03,  9.55it/s]\n",
      "{'macro_f1': 0.0295, 'weighted_f1': 0.2466}\n",
      "epoch 10 , step 29 , loss: 63.1874: : 30it [00:03,  9.02it/s]\n",
      "{'macro_f1': 0.0295, 'weighted_f1': 0.2466}\n",
      "epoch 11 , step 29 , loss: 63.1412: : 30it [00:03,  9.66it/s]\n",
      "{'macro_f1': 0.0295, 'weighted_f1': 0.2466}\n",
      "epoch 12 , step 29 , loss: 63.3994: : 30it [00:03,  9.89it/s]\n",
      "{'macro_f1': 0.0295, 'weighted_f1': 0.2466}\n",
      "epoch 13 , step 29 , loss: 63.3088: : 30it [00:03,  9.72it/s]\n",
      "{'macro_f1': 0.0295, 'weighted_f1': 0.2466}\n",
      "epoch 14 , step 29 , loss: 63.4935: : 30it [00:03,  9.09it/s]\n",
      "{'macro_f1': 0.0295, 'weighted_f1': 0.2466}\n",
      "epoch 15 , step 29 , loss: 63.5465: : 30it [00:03,  9.24it/s]\n",
      "{'macro_f1': 0.0295, 'weighted_f1': 0.2466}\n",
      "epoch 16 , step 29 , loss: 63.0683: : 30it [00:03,  9.54it/s]\n",
      "{'macro_f1': 0.0295, 'weighted_f1': 0.2466}\n",
      "epoch 17 , step 29 , loss: 63.4833: : 30it [00:03,  9.76it/s]\n",
      "{'macro_f1': 0.0295, 'weighted_f1': 0.2466}\n",
      "epoch 18 , step 29 , loss: 63.7748: : 30it [00:03,  9.37it/s]\n",
      "{'macro_f1': 0.0295, 'weighted_f1': 0.2466}\n",
      "epoch 19 , step 29 , loss: 63.6194: : 30it [00:03,  9.84it/s]\n",
      "{'macro_f1': 0.0295, 'weighted_f1': 0.2466}\n",
      "epoch 20 , step 29 , loss: 62.8709: : 30it [00:03,  9.46it/s]\n",
      "{'macro_f1': 0.0295, 'weighted_f1': 0.2466}\n",
      "epoch 21 , step 29 , loss: 62.9355: : 30it [00:03,  9.90it/s]\n",
      "{'macro_f1': 0.0295, 'weighted_f1': 0.2466}\n",
      "epoch 22 , step 29 , loss: 63.1837: : 30it [00:03,  9.10it/s]\n",
      "{'macro_f1': 0.0295, 'weighted_f1': 0.2466}\n",
      "epoch 23 , step 29 , loss: 62.9461: : 30it [00:03,  9.39it/s]\n",
      "{'macro_f1': 0.0295, 'weighted_f1': 0.2466}\n",
      "epoch 24 , step 29 , loss: 62.8628: : 30it [00:03,  9.82it/s]\n",
      "{'macro_f1': 0.0295, 'weighted_f1': 0.2466}\n",
      "epoch 25 , step 29 , loss: 62.9598: : 30it [00:03,  9.61it/s]\n",
      "{'macro_f1': 0.0295, 'weighted_f1': 0.2466}\n"
     ]
    }
   ],
   "source": [
    "bert_model = train(hparams, bert_model, loaders, lr=hparams['learning_rate'], schedule=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(bert_model, loaders[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}