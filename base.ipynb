{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitnnconda048fc75fe4ee43f1aa97608c8881ebba",
   "display_name": "Python 3.8.5 64-bit ('nn': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.base_model import BiLSTM_CRF\n",
    "from utils.utils import prepare, predict, train, evaluate\n",
    "\n",
    "hparams = {\n",
    "    'path':'data/data.txt',\n",
    "    'epochs': 80,\n",
    "    'batch_size': 100,\n",
    "    'embedding_dim': 300,\n",
    "    'hidden_dim': 256,\n",
    "    'device':'cuda:1',\n",
    "    'seq_length': 256,\n",
    "    'save_path':'model_params/base_model.model'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_dict, loaders = prepare(hparams)\n",
    "hparams['vocab_size'] = len(attr_dict['tokenizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = BiLSTM_CRF(hparams, attr_dict['tag2idx']).to(hparams['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch 1 , step 8 , loss: 447.5368: : 9it [00:02,  3.62it/s]\n",
      "{'macro_f1': 0.0768, 'weighted_f1': 0.7796}\n",
      "epoch 2 , step 8 , loss: 270.1000: : 9it [00:02,  3.51it/s]\n",
      "{'macro_f1': 0.0765, 'weighted_f1': 0.7771}\n",
      "epoch 3 , step 8 , loss: 239.7688: : 9it [00:02,  3.66it/s]\n",
      "{'macro_f1': 0.0778, 'weighted_f1': 0.7823}\n",
      "epoch 4 , step 8 , loss: 199.2411: : 9it [00:02,  3.45it/s]\n",
      "{'macro_f1': 0.1167, 'weighted_f1': 0.8271}\n",
      "epoch 5 , step 8 , loss: 170.4554: : 9it [00:02,  3.68it/s]\n",
      "{'macro_f1': 0.1421, 'weighted_f1': 0.8413}\n",
      "epoch 6 , step 8 , loss: 152.2573: : 9it [00:02,  3.29it/s]\n",
      "{'macro_f1': 0.1614, 'weighted_f1': 0.8503}\n",
      "epoch 7 , step 8 , loss: 139.1526: : 9it [00:02,  3.37it/s]\n",
      "{'macro_f1': 0.1754, 'weighted_f1': 0.8556}\n",
      "epoch 8 , step 8 , loss: 129.6469: : 9it [00:02,  3.33it/s]\n",
      "{'macro_f1': 0.2035, 'weighted_f1': 0.8595}\n",
      "epoch 9 , step 8 , loss: 120.0105: : 9it [00:02,  3.38it/s]\n",
      "{'macro_f1': 0.2314, 'weighted_f1': 0.8659}\n",
      "epoch 10 , step 8 , loss: 110.3648: : 9it [00:02,  3.62it/s]\n",
      "{'macro_f1': 0.2597, 'weighted_f1': 0.872}\n",
      "epoch 11 , step 8 , loss: 101.2071: : 9it [00:02,  3.73it/s]\n",
      "{'macro_f1': 0.3017, 'weighted_f1': 0.8839}\n",
      "epoch 12 , step 8 , loss: 91.2273: : 9it [00:02,  3.31it/s]\n",
      "{'macro_f1': 0.344, 'weighted_f1': 0.8928}\n",
      "epoch 13 , step 8 , loss: 81.9736: : 9it [00:02,  3.62it/s]\n",
      "{'macro_f1': 0.3762, 'weighted_f1': 0.8984}\n",
      "epoch 14 , step 8 , loss: 73.9376: : 9it [00:02,  3.39it/s]\n",
      "{'macro_f1': 0.3898, 'weighted_f1': 0.9018}\n",
      "epoch 15 , step 8 , loss: 66.5978: : 9it [00:02,  3.60it/s]\n",
      "{'macro_f1': 0.4229, 'weighted_f1': 0.9072}\n",
      "epoch 16 , step 8 , loss: 61.3216: : 9it [00:02,  3.74it/s]\n",
      "{'macro_f1': 0.4434, 'weighted_f1': 0.9098}\n",
      "epoch 17 , step 8 , loss: 55.5650: : 9it [00:02,  3.66it/s]\n",
      "{'macro_f1': 0.4569, 'weighted_f1': 0.915}\n",
      "epoch 18 , step 8 , loss: 50.8622: : 9it [00:02,  3.66it/s]\n",
      "{'macro_f1': 0.4724, 'weighted_f1': 0.9175}\n",
      "epoch 19 , step 8 , loss: 46.5581: : 9it [00:02,  3.61it/s]\n",
      "{'macro_f1': 0.4995, 'weighted_f1': 0.9221}\n",
      "epoch 20 , step 8 , loss: 42.8683: : 9it [00:02,  3.69it/s]\n",
      "{'macro_f1': 0.5199, 'weighted_f1': 0.9243}\n",
      "epoch 21 , step 8 , loss: 39.2390: : 9it [00:02,  3.63it/s]\n",
      "{'macro_f1': 0.541, 'weighted_f1': 0.9259}\n",
      "epoch 22 , step 8 , loss: 36.1582: : 9it [00:02,  3.53it/s]\n",
      "{'macro_f1': 0.5557, 'weighted_f1': 0.9288}\n",
      "epoch 23 , step 8 , loss: 33.4480: : 9it [00:02,  3.71it/s]\n",
      "{'macro_f1': 0.575, 'weighted_f1': 0.9295}\n",
      "epoch 24 , step 8 , loss: 31.0143: : 9it [00:02,  3.61it/s]\n",
      "{'macro_f1': 0.588, 'weighted_f1': 0.932}\n",
      "epoch 25 , step 8 , loss: 29.0027: : 9it [00:02,  3.33it/s]\n",
      "{'macro_f1': 0.5985, 'weighted_f1': 0.932}\n",
      "epoch 26 , step 8 , loss: 27.1477: : 9it [00:02,  3.66it/s]\n",
      "{'macro_f1': 0.6183, 'weighted_f1': 0.9338}\n",
      "epoch 27 , step 8 , loss: 27.2093: : 9it [00:02,  3.52it/s]\n",
      "{'macro_f1': 0.6072, 'weighted_f1': 0.9321}\n",
      "epoch 28 , step 8 , loss: 29.4809: : 9it [00:02,  3.69it/s]\n",
      "{'macro_f1': 0.5967, 'weighted_f1': 0.9288}\n",
      "epoch 29 , step 8 , loss: 29.4137: : 9it [00:02,  3.35it/s]\n",
      "{'macro_f1': 0.6213, 'weighted_f1': 0.9354}\n",
      "epoch 30 , step 8 , loss: 25.7714: : 9it [00:02,  3.68it/s]\n",
      "{'macro_f1': 0.6245, 'weighted_f1': 0.9365}\n",
      "epoch 31 , step 8 , loss: 23.2977: : 9it [00:02,  3.71it/s]\n",
      "{'macro_f1': 0.6463, 'weighted_f1': 0.9406}\n",
      "epoch 32 , step 8 , loss: 21.4521: : 9it [00:02,  3.69it/s]\n",
      "{'macro_f1': 0.6541, 'weighted_f1': 0.9439}\n",
      "epoch 33 , step 8 , loss: 20.0666: : 9it [00:02,  3.66it/s]\n",
      "{'macro_f1': 0.6562, 'weighted_f1': 0.9422}\n",
      "epoch 34 , step 8 , loss: 18.8730: : 9it [00:02,  3.67it/s]\n",
      "{'macro_f1': 0.6658, 'weighted_f1': 0.9429}\n",
      "epoch 35 , step 8 , loss: 17.8838: : 9it [00:02,  3.74it/s]\n",
      "{'macro_f1': 0.6702, 'weighted_f1': 0.9441}\n",
      "epoch 36 , step 8 , loss: 16.9704: : 9it [00:02,  3.50it/s]\n",
      "{'macro_f1': 0.669, 'weighted_f1': 0.9437}\n",
      "epoch 37 , step 8 , loss: 16.1365: : 9it [00:02,  3.56it/s]\n",
      "{'macro_f1': 0.6754, 'weighted_f1': 0.9435}\n",
      "epoch 38 , step 8 , loss: 15.3579: : 9it [00:02,  3.69it/s]\n",
      "{'macro_f1': 0.6766, 'weighted_f1': 0.9443}\n",
      "epoch 39 , step 8 , loss: 14.8290: : 9it [00:02,  3.61it/s]\n",
      "{'macro_f1': 0.679, 'weighted_f1': 0.9446}\n",
      "epoch 40 , step 8 , loss: 14.1039: : 9it [00:02,  3.47it/s]\n",
      "{'macro_f1': 0.6863, 'weighted_f1': 0.9431}\n",
      "epoch 41 , step 8 , loss: 13.4892: : 9it [00:02,  3.68it/s]\n",
      "{'macro_f1': 0.6784, 'weighted_f1': 0.9429}\n",
      "epoch 42 , step 8 , loss: 12.8476: : 9it [00:02,  3.57it/s]\n",
      "{'macro_f1': 0.69, 'weighted_f1': 0.9442}\n",
      "epoch 43 , step 8 , loss: 12.3487: : 9it [00:02,  3.46it/s]\n",
      "{'macro_f1': 0.6907, 'weighted_f1': 0.9459}\n",
      "epoch 44 , step 8 , loss: 11.8361: : 9it [00:02,  3.66it/s]\n",
      "{'macro_f1': 0.6928, 'weighted_f1': 0.9465}\n",
      "epoch 45 , step 8 , loss: 11.3707: : 9it [00:02,  3.44it/s]\n",
      "{'macro_f1': 0.693, 'weighted_f1': 0.9464}\n",
      "epoch 46 , step 8 , loss: 11.0383: : 9it [00:02,  3.33it/s]\n",
      "{'macro_f1': 0.6979, 'weighted_f1': 0.9473}\n",
      "epoch 47 , step 8 , loss: 10.5074: : 9it [00:02,  3.66it/s]\n",
      "{'macro_f1': 0.6855, 'weighted_f1': 0.9447}\n",
      "epoch 48 , step 8 , loss: 10.1070: : 9it [00:02,  3.57it/s]\n",
      "{'macro_f1': 0.7072, 'weighted_f1': 0.9465}\n",
      "epoch 49 , step 8 , loss: 9.6492: : 9it [00:02,  3.69it/s]\n",
      "{'macro_f1': 0.7111, 'weighted_f1': 0.9469}\n",
      "epoch 50 , step 8 , loss: 9.2816: : 9it [00:02,  3.54it/s]\n",
      "{'macro_f1': 0.7198, 'weighted_f1': 0.9495}\n",
      "epoch 51 , step 8 , loss: 8.8975: : 9it [00:02,  3.57it/s]\n",
      "{'macro_f1': 0.7067, 'weighted_f1': 0.9466}\n",
      "epoch 52 , step 8 , loss: 8.5601: : 9it [00:02,  3.72it/s]\n",
      "{'macro_f1': 0.7146, 'weighted_f1': 0.9471}\n",
      "epoch 53 , step 8 , loss: 8.2128: : 9it [00:02,  3.56it/s]\n",
      "{'macro_f1': 0.7183, 'weighted_f1': 0.9484}\n",
      "epoch 54 , step 8 , loss: 7.8971: : 9it [00:02,  3.61it/s]\n",
      "{'macro_f1': 0.7238, 'weighted_f1': 0.9481}\n",
      "epoch 55 , step 8 , loss: 7.6006: : 9it [00:02,  3.66it/s]\n",
      "{'macro_f1': 0.7255, 'weighted_f1': 0.9484}\n",
      "epoch 56 , step 8 , loss: 7.3429: : 9it [00:02,  3.40it/s]\n",
      "{'macro_f1': 0.7327, 'weighted_f1': 0.9506}\n",
      "epoch 57 , step 8 , loss: 7.1247: : 9it [00:02,  3.52it/s]\n",
      "{'macro_f1': 0.7241, 'weighted_f1': 0.9469}\n",
      "epoch 58 , step 8 , loss: 6.8769: : 9it [00:02,  3.46it/s]\n",
      "{'macro_f1': 0.7214, 'weighted_f1': 0.9465}\n",
      "epoch 59 , step 8 , loss: 6.6985: : 9it [00:02,  3.66it/s]\n",
      "{'macro_f1': 0.7259, 'weighted_f1': 0.9495}\n",
      "epoch 60 , step 8 , loss: 6.5156: : 9it [00:02,  3.49it/s]\n",
      "{'macro_f1': 0.7404, 'weighted_f1': 0.9498}\n",
      "epoch 61 , step 8 , loss: 6.2450: : 9it [00:02,  3.57it/s]\n",
      "{'macro_f1': 0.7299, 'weighted_f1': 0.9485}\n",
      "epoch 62 , step 8 , loss: 6.0502: : 9it [00:02,  3.65it/s]\n",
      "{'macro_f1': 0.746, 'weighted_f1': 0.9487}\n",
      "epoch 63 , step 8 , loss: 6.0749: : 9it [00:02,  3.59it/s]\n",
      "{'macro_f1': 0.7338, 'weighted_f1': 0.9448}\n",
      "epoch 64 , step 8 , loss: 6.7583: : 9it [00:02,  3.61it/s]\n",
      "{'macro_f1': 0.7186, 'weighted_f1': 0.944}\n",
      "epoch 65 , step 8 , loss: 6.3752: : 9it [00:02,  3.69it/s]\n",
      "{'macro_f1': 0.7414, 'weighted_f1': 0.9454}\n",
      "epoch 66 , step 8 , loss: 5.9616: : 9it [00:02,  3.60it/s]\n",
      "{'macro_f1': 0.7181, 'weighted_f1': 0.9457}\n",
      "epoch 67 , step 8 , loss: 5.6802: : 9it [00:02,  3.64it/s]\n",
      "{'macro_f1': 0.7355, 'weighted_f1': 0.9463}\n",
      "epoch 68 , step 8 , loss: 5.3635: : 9it [00:02,  3.52it/s]\n",
      "{'macro_f1': 0.7321, 'weighted_f1': 0.9473}\n",
      "epoch 69 , step 8 , loss: 5.1339: : 9it [00:02,  3.57it/s]\n",
      "{'macro_f1': 0.7409, 'weighted_f1': 0.9469}\n",
      "epoch 70 , step 8 , loss: 4.8688: : 9it [00:02,  3.54it/s]\n",
      "{'macro_f1': 0.7414, 'weighted_f1': 0.9473}\n",
      "epoch 71 , step 8 , loss: 4.7160: : 9it [00:02,  3.67it/s]\n",
      "{'macro_f1': 0.7495, 'weighted_f1': 0.9488}\n",
      "epoch 72 , step 8 , loss: 4.5219: : 9it [00:02,  3.64it/s]\n",
      "{'macro_f1': 0.7431, 'weighted_f1': 0.948}\n",
      "epoch 73 , step 8 , loss: 4.3949: : 9it [00:02,  3.60it/s]\n",
      "{'macro_f1': 0.7427, 'weighted_f1': 0.9482}\n",
      "epoch 74 , step 8 , loss: 4.2426: : 9it [00:02,  3.64it/s]\n",
      "{'macro_f1': 0.743, 'weighted_f1': 0.9465}\n",
      "epoch 75 , step 8 , loss: 4.1227: : 9it [00:02,  3.43it/s]\n",
      "{'macro_f1': 0.7429, 'weighted_f1': 0.9468}\n",
      "epoch 76 , step 8 , loss: 3.9658: : 9it [00:02,  3.25it/s]\n",
      "{'macro_f1': 0.7426, 'weighted_f1': 0.9463}\n",
      "epoch 77 , step 8 , loss: 3.8609: : 9it [00:02,  3.55it/s]\n",
      "{'macro_f1': 0.7449, 'weighted_f1': 0.9468}\n",
      "epoch 78 , step 8 , loss: 3.7771: : 9it [00:02,  3.49it/s]\n",
      "{'macro_f1': 0.7433, 'weighted_f1': 0.9471}\n",
      "epoch 79 , step 8 , loss: 3.6682: : 9it [00:02,  3.90it/s]\n",
      "{'macro_f1': 0.7468, 'weighted_f1': 0.9479}\n",
      "epoch 80 , step 8 , loss: 3.5421: : 9it [00:02,  3.94it/s]\n",
      "{'macro_f1': 0.743, 'weighted_f1': 0.9474}\n"
     ]
    }
   ],
   "source": [
    "base_model = train(hparams, base_model, loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(base_model, loaders[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['B-department',\n",
       "  'I-department',\n",
       "  'I-department',\n",
       "  'I-department',\n",
       "  'I-department',\n",
       "  'I-department',\n",
       "  'O',\n",
       "  'O',\n",
       "  'I-scholarship',\n",
       "  'I-scholarship',\n",
       "  'I-scholarship',\n",
       "  'I-scholarship',\n",
       "  'I-scholarship',\n",
       "  'I-scholarship',\n",
       "  'I-scholarship',\n",
       "  'I-scholarship',\n",
       "  'I-scholarship',\n",
       "  'I-scholarship',\n",
       "  'I-scholarship',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-department',\n",
       "  'I-department',\n",
       "  'I-department',\n",
       "  'I-organization',\n",
       "  'I-organization',\n",
       "  'I-organization',\n",
       "  'I-organization',\n",
       "  'I-organization',\n",
       "  'I-organization',\n",
       "  'I-organization',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'I-organization',\n",
       "  'I-department',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]']]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "predict(['中国人民大学第三十五届一二九合唱音乐节如期举行，信息学院分团委文化部将组织信院全体同学参加'], base_model, attr_dict['tokenizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(base_model.state_dict(), hparams['save_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}