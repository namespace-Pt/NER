{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitnnconda048fc75fe4ee43f1aa97608c8881ebba",
   "display_name": "Python 3.8.5 64-bit ('nn': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from model import BiLSTM_CRF\n",
    "from utils import prepare, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'path':'/home/peitian_zhang/Data/NER/labeled_train.txt',\n",
    "    'epochs': 150,\n",
    "    'batch_size': 100,\n",
    "    'embedding_dim': 300,\n",
    "    'hidden_dim': 150,\n",
    "    'device':'cuda:0',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'<START>': 0,\n",
       " '<END>': 1,\n",
       " '<PAD>': 2,\n",
       " 'O': 3,\n",
       " 'B-C': 4,\n",
       " 'I-C': 5,\n",
       " 'B-A': 6,\n",
       " 'I-A': 7,\n",
       " 'B-O': 8,\n",
       " 'I-O': 9,\n",
       " 'B-M': 10,\n",
       " 'I-M': 11,\n",
       " 'B-P': 12,\n",
       " 'I-P': 13,\n",
       " 'B-N': 14,\n",
       " 'I-N': 15,\n",
       " 'B-D': 16,\n",
       " 'I-D': 17,\n",
       " 'B-S': 18,\n",
       " 'I-S': 19,\n",
       " 'B-L': 20,\n",
       " 'I-L': 21}"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "tag2idx, vocab, loader = prepare(hparams)\n",
    "hparams['vocab_size'] = len(vocab)\n",
    "hparams['seq_length'] = loader.dataset.max_length\n",
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM_CRF(hparams, tag2idx).to(hparams['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 9, 11, 10,  ..., 11, 10,  8],\n        [ 9, 11, 10,  ..., 11, 10,  8],\n        [ 9, 11, 10,  ...,  8,  6,  8],\n        ...,\n        [ 9, 11, 10,  ..., 11, 10,  8],\n        [ 9, 11, 10,  ..., 11, 10,  8],\n        [ 9, 11, 10,  ..., 11, 10,  8]], device='cuda:0') tensor([[ 3,  3,  3,  ...,  2,  2,  2],\n        [ 3,  3,  3,  ...,  2,  2,  2],\n        [14, 15, 15,  ...,  2,  2,  2],\n        ...,\n        [ 3,  3,  3,  ...,  2,  2,  2],\n        [ 8,  9,  9,  ...,  2,  2,  2],\n        [16, 17, 17,  ...,  2,  2,  2]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    record = next(iter(loader))\n",
    "    _, tag_seq = model(record['token'])\n",
    "    print(tag_seq, record['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch 1 , step 9 , loss: 1421.0073: : 10it [00:03,  3.29it/s]\n",
      "epoch 2 , step 9 , loss: 899.5295: : 10it [00:02,  3.37it/s]\n",
      "epoch 3 , step 9 , loss: 295.0562: : 10it [00:02,  3.37it/s]\n",
      "epoch 4 , step 9 , loss: -333.2220: : 10it [00:02,  3.36it/s]\n",
      "epoch 5 , step 9 , loss: -970.6711: : 10it [00:02,  3.44it/s]\n",
      "epoch 6 , step 9 , loss: -1623.1125: : 10it [00:02,  3.38it/s]\n",
      "epoch 7 , step 9 , loss: -2296.8902: : 10it [00:02,  3.34it/s]\n",
      "epoch 8 , step 9 , loss: -2991.2155: : 10it [00:02,  3.35it/s]\n",
      "epoch 9 , step 9 , loss: -3682.3815: : 10it [00:02,  3.41it/s]\n",
      "epoch 10 , step 9 , loss: -4383.0485: : 10it [00:02,  3.38it/s]\n",
      "epoch 11 , step 9 , loss: -5080.5594: : 10it [00:02,  3.43it/s]\n",
      "epoch 12 , step 9 , loss: -5774.6074: : 10it [00:02,  3.43it/s]\n",
      "epoch 13 , step 9 , loss: -6459.7176: : 10it [00:02,  3.39it/s]\n",
      "epoch 14 , step 9 , loss: -7135.3322: : 10it [00:02,  3.47it/s]\n",
      "epoch 15 , step 9 , loss: -7804.2098: : 10it [00:03,  3.27it/s]\n",
      "epoch 16 , step 9 , loss: -8465.2892: : 10it [00:02,  3.49it/s]\n",
      "epoch 17 , step 9 , loss: -9124.9841: : 10it [00:02,  3.48it/s]\n",
      "epoch 18 , step 9 , loss: -9778.8167: : 10it [00:02,  3.45it/s]\n",
      "epoch 19 , step 9 , loss: -10430.6914: : 10it [00:02,  3.41it/s]\n",
      "epoch 20 , step 9 , loss: -11078.7144: : 10it [00:03,  3.33it/s]\n",
      "epoch 21 , step 9 , loss: -11722.1103: : 10it [00:02,  3.50it/s]\n",
      "epoch 22 , step 9 , loss: -12357.4573: : 10it [00:02,  3.46it/s]\n",
      "epoch 23 , step 9 , loss: -12997.2469: : 10it [00:02,  3.47it/s]\n",
      "epoch 24 , step 9 , loss: -13636.6171: : 10it [00:02,  3.39it/s]\n",
      "epoch 25 , step 9 , loss: -14274.9849: : 10it [00:02,  3.46it/s]\n",
      "epoch 26 , step 9 , loss: -14908.7715: : 10it [00:02,  3.44it/s]\n",
      "epoch 27 , step 9 , loss: -15543.4355: : 10it [00:02,  3.46it/s]\n",
      "epoch 28 , step 9 , loss: -16173.5563: : 10it [00:02,  3.46it/s]\n",
      "epoch 29 , step 9 , loss: -16802.0330: : 10it [00:02,  3.48it/s]\n",
      "epoch 30 , step 9 , loss: -17432.5295: : 10it [00:02,  3.45it/s]\n",
      "epoch 31 , step 9 , loss: -18058.8254: : 10it [00:02,  3.43it/s]\n",
      "epoch 32 , step 9 , loss: -18682.0822: : 10it [00:02,  3.40it/s]\n",
      "epoch 33 , step 9 , loss: -19306.4156: : 10it [00:02,  3.39it/s]\n",
      "epoch 34 , step 9 , loss: -19928.1012: : 10it [00:02,  3.44it/s]\n",
      "epoch 35 , step 9 , loss: -20547.6719: : 10it [00:02,  3.43it/s]\n",
      "epoch 36 , step 9 , loss: -21165.9877: : 10it [00:03,  3.21it/s]\n",
      "epoch 37 , step 9 , loss: -21780.2596: : 10it [00:03,  3.22it/s]\n",
      "epoch 38 , step 9 , loss: -22392.5584: : 10it [00:02,  3.43it/s]\n",
      "epoch 39 , step 9 , loss: -23014.4443: : 10it [00:02,  3.35it/s]\n",
      "epoch 40 , step 9 , loss: -23641.7244: : 10it [00:02,  3.44it/s]\n",
      "epoch 41 , step 9 , loss: -24266.4049: : 10it [00:02,  3.40it/s]\n",
      "epoch 42 , step 9 , loss: -24887.7080: : 10it [00:02,  3.46it/s]\n",
      "epoch 43 , step 9 , loss: -25503.0006: : 10it [00:02,  3.34it/s]\n",
      "epoch 44 , step 9 , loss: -26120.6098: : 10it [00:03,  3.19it/s]\n",
      "epoch 45 , step 9 , loss: -26740.1502: : 10it [00:03,  3.00it/s]\n",
      "epoch 46 , step 9 , loss: -27359.3078: : 10it [00:03,  3.05it/s]\n",
      "epoch 47 , step 9 , loss: -27982.0418: : 10it [00:03,  3.23it/s]\n",
      "epoch 48 , step 9 , loss: -28602.1133: : 10it [00:03,  3.29it/s]\n",
      "epoch 49 , step 9 , loss: -29214.3305: : 10it [00:03,  3.12it/s]\n",
      "epoch 50 , step 9 , loss: -29824.7006: : 10it [00:02,  3.46it/s]\n",
      "epoch 51 , step 9 , loss: -30432.3793: : 10it [00:02,  3.48it/s]\n",
      "epoch 52 , step 9 , loss: -31047.3879: : 10it [00:02,  3.54it/s]\n",
      "epoch 53 , step 9 , loss: -31657.3086: : 10it [00:02,  3.44it/s]\n",
      "epoch 54 , step 9 , loss: -32267.5297: : 10it [00:02,  3.37it/s]\n",
      "epoch 55 , step 9 , loss: -32883.4088: : 10it [00:03,  3.33it/s]\n",
      "epoch 56 , step 9 , loss: -33499.9062: : 10it [00:02,  3.51it/s]\n",
      "epoch 57 , step 9 , loss: -34116.7215: : 10it [00:02,  3.42it/s]\n",
      "epoch 58 , step 9 , loss: -34729.8551: : 10it [00:02,  3.43it/s]\n",
      "epoch 59 , step 9 , loss: -35343.6953: : 10it [00:02,  3.39it/s]\n",
      "epoch 60 , step 9 , loss: -35956.4055: : 10it [00:02,  3.38it/s]\n",
      "epoch 61 , step 9 , loss: -36571.0813: : 10it [00:02,  3.44it/s]\n",
      "epoch 62 , step 9 , loss: -37178.8457: : 10it [00:02,  3.46it/s]\n",
      "epoch 63 , step 9 , loss: -37791.4992: : 10it [00:02,  3.42it/s]\n",
      "epoch 64 , step 9 , loss: -38403.5492: : 10it [00:02,  3.53it/s]\n",
      "epoch 65 , step 9 , loss: -39017.3391: : 10it [00:02,  3.44it/s]\n",
      "epoch 66 , step 9 , loss: -39634.0508: : 10it [00:02,  3.45it/s]\n",
      "epoch 67 , step 9 , loss: -40245.3516: : 10it [00:02,  3.46it/s]\n",
      "epoch 68 , step 9 , loss: -40859.9820: : 10it [00:02,  3.43it/s]\n",
      "epoch 69 , step 9 , loss: -41474.9617: : 10it [00:02,  3.51it/s]\n",
      "epoch 70 , step 9 , loss: -42089.3113: : 10it [00:02,  3.48it/s]\n",
      "epoch 71 , step 9 , loss: -42704.1000: : 10it [00:03,  3.28it/s]\n",
      "epoch 72 , step 9 , loss: -43317.4629: : 10it [00:02,  3.38it/s]\n",
      "epoch 73 , step 9 , loss: -43930.7391: : 10it [00:02,  3.42it/s]\n",
      "epoch 74 , step 9 , loss: -44540.3168: : 10it [00:02,  3.41it/s]\n",
      "epoch 75 , step 9 , loss: -45145.0152: : 10it [00:02,  3.43it/s]\n",
      "epoch 76 , step 9 , loss: -45743.7492: : 10it [00:02,  3.46it/s]\n",
      "epoch 77 , step 9 , loss: -46356.1934: : 10it [00:02,  3.44it/s]\n",
      "epoch 78 , step 9 , loss: -46967.3797: : 10it [00:02,  3.45it/s]\n",
      "epoch 79 , step 9 , loss: -47584.8203: : 10it [00:02,  3.47it/s]\n",
      "epoch 80 , step 9 , loss: -48196.6684: : 10it [00:02,  3.47it/s]\n",
      "epoch 81 , step 9 , loss: -48821.0336: : 10it [00:02,  3.47it/s]\n",
      "epoch 82 , step 9 , loss: -49444.0754: : 10it [00:02,  3.40it/s]\n",
      "epoch 83 , step 9 , loss: -50057.2820: : 10it [00:02,  3.40it/s]\n",
      "epoch 84 , step 9 , loss: -50664.4555: : 10it [00:02,  3.46it/s]\n",
      "epoch 85 , step 9 , loss: -51272.7027: : 10it [00:02,  3.52it/s]\n",
      "epoch 86 , step 9 , loss: -51871.0805: : 10it [00:02,  3.46it/s]\n",
      "epoch 87 , step 9 , loss: -52470.8855: : 10it [00:02,  3.50it/s]\n",
      "epoch 88 , step 9 , loss: -53077.6461: : 10it [00:02,  3.50it/s]\n",
      "epoch 89 , step 9 , loss: -53687.3094: : 10it [00:02,  3.41it/s]\n",
      "epoch 90 , step 9 , loss: -54291.0066: : 10it [00:03,  3.11it/s]\n",
      "epoch 91 , step 9 , loss: -54899.4285: : 10it [00:03,  3.27it/s]\n",
      "epoch 92 , step 9 , loss: -55509.5844: : 10it [00:03,  3.14it/s]\n",
      "epoch 93 , step 9 , loss: -56113.4238: : 10it [00:02,  3.40it/s]\n",
      "epoch 94 , step 9 , loss: -56723.0164: : 10it [00:02,  3.46it/s]\n",
      "epoch 95 , step 9 , loss: -57329.8273: : 10it [00:02,  3.43it/s]\n",
      "epoch 96 , step 9 , loss: -57942.1457: : 10it [00:02,  3.39it/s]\n",
      "epoch 97 , step 9 , loss: -58561.4453: : 10it [00:02,  3.47it/s]\n",
      "epoch 98 , step 9 , loss: -59169.5711: : 10it [00:02,  3.42it/s]\n",
      "epoch 99 , step 9 , loss: -59777.8332: : 10it [00:02,  3.42it/s]\n",
      "epoch 100 , step 9 , loss: -60381.0523: : 10it [00:02,  3.40it/s]\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "for epoch in range(hparams['epochs']):\n",
    "    tqdm_ = tqdm(enumerate(loader))\n",
    "    total_loss = 0\n",
    "\n",
    "    for step,x in tqdm_:\n",
    "        model.zero_grad()\n",
    "        loss = model.neg_log_likelihood(x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        tqdm_.set_description(\"epoch {:d} , step {:d} , loss: {:.4f}\".format(epoch+1, step, total_loss/(step+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prediction:tensor([[ 3,  3,  3,  ...,  2,  2,  2],\n        [ 3,  3,  3,  ...,  2,  2,  2],\n        [ 5, 15,  3,  ...,  2,  2,  2],\n        ...,\n        [ 3,  3,  3,  ...,  2,  2,  2],\n        [ 9,  9,  9,  ...,  2,  2,  2],\n        [12, 16, 17,  ...,  2,  2,  2]], device='cuda:0')\n Ground Truth:tensor([[ 3,  3,  3,  ...,  2,  2,  2],\n        [ 3,  3,  3,  ...,  2,  2,  2],\n        [14, 15, 15,  ...,  2,  2,  2],\n        ...,\n        [ 3,  3,  3,  ...,  2,  2,  2],\n        [ 8,  9,  9,  ...,  2,  2,  2],\n        [16, 17, 17,  ...,  2,  2,  2]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    record = next(iter(loader))\n",
    "    _, tag_seq = model(record['token'])\n",
    "    print(\"Prediction:{}\\n Ground Truth:{}\".format(tag_seq, record['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>']]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "predict(['窦志成获奖'],model,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}